{
    "meta_data": {
        "title": "1-to-K Contrastive Learning for Consistent Cross-lingual Cross-modal Retrieval",
        "authors": [
            "Jiawei Ni",
            "Ru Li",
            "Wenhan Luo",
            "Fei Wu"
        ],
        "affiliations": [
            "Zhejiang University, China"
        ],
        "abstract": "Cross-lingual Cross-modal Retrieval (CCR) focuses on achieving image-text retrieval in multilingual scenarios using a single model. Existing methods primarily rely on pairwise contrastive learning, leading to inconsistency problems in retrieval performance across languages. This paper proposes a novel 1-to-K contrastive learning framework for CCR that effectively aligns images with texts across multiple languages simultaneously. Our approach eliminates intra-modal error propagation and inter-modal optimization bias. Extensive experiments on four popular CCR datasets show our method achieves state-of-the-art performance and improves consistency across languages.",
        "keywords": [
            "cross-lingual",
            "cross-modal",
            "contrastive learning",
            "retrieval",
            "multilingual"
        ],
        "year": "2022",
        "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
        "doi link": "https://doi.org/10.1234567/abcde",
        "method name": "1-to-K Contrastive Learning"
    },
    "relate work": {
        "related work category": [
            "Cross-Lingual Cross-Modal Pre-Training",
            "Cross-Lingual Cross-Modal Retrieval"
        ],
        "related papers": "\\cite{devlin2018bert,conneau2019cross,chi2021infoxlm,radford2021learning,li2021align,ni2021m3p,zhou2021uc2,shan2022ernie,zeng2022cross}",
        "comparisons with related methods": "The proposed method outperforms existing pairwise contrastive learning methods by addressing intra-modal error propagation and ensuring consistent performance across languages. It surpasses leading models like CLIP, xUNITER, UC$^2$, and others in Recall@K across multiple CCR datasets."
    },
    "high_level_summary": {
        "summary of this paper": "This paper introduces a 1-to-K contrastive learning framework for cross-lingual cross-modal retrieval, addressing inconsistency challenges of traditional pairwise contrastive methods. Extensive experiments validate the superior performance of this approach across multiple datasets.",
        "research purpose": "To achieve consistent cross-lingual cross-modal retrieval by eliminating performance inconsistencies in existing methods.",
        "research challenge": "Existing methods suffer from intra-modal error propagation and inter-modal optimization directional bias, leading to inconsistent performance across languages.",
        "method summary": "The 1-to-K contrastive learning framework aligns images with texts across multiple languages simultaneously during pre-training, ensuring consistent cross-lingual cross-modal retrieval.",
        "conclusion": "The 1-to-K contrastive learning framework effectively addresses inconsistency issues, leading to state-of-the-art results in cross-lingual cross-modal retrieval tasks."
    },
    "Method": {
        "description": "The CCR$^k$ model uses a 1-to-K contrastive learning approach, aligning each image with multiple language texts simultaneously to enhance inter- and intra-modal consistency.",
        "problem formultaion": "Traditional pairwise contrastive learning methods result in language performance inconsistency in CCR.",
        "feature processing": "Not specifically addressed; the method focuses on contrastive alignment of multi-lingual representations for CCR.",
        "model": "CCR$^k$, featuring a novel contrastive framework that incorporates 1-to-K contrastive learning.",
        "tasks": [
            "Cross-lingual Cross-Modal Retrieval"
        ],
        "theoretical analysis": "Analyzed the limitations of current contrastive methods in aligning multilingual representations and formulated the effectiveness of 1-to-K contrastive learning.",
        "complexity": "Efficient implementation of 1-to-K framework relative to baseline methods, utilizing shared attention mechanisms to optimize computation.",
        "algorithm step": "Align images with multiple language texts in a 1-to-K configuration instead of pairwise, mitigating direction bias and error propagation."
    },
    "Experiments": {
        "datasets": [
            "Conceptual Captions 3M",
            "SBU Caption",
            "Visual Genome",
            "COCO",
            "xFlickr&CO",
            "WIT",
            "Multi30K"
        ],
        "baselines": [
            "xUNITER",
            "UC$^2$",
            "M$^3$P",
            "TD-MML",
            "CCLM"
        ],
        "evaluation metric": "Recall@K, Mean Rank Variance (MRV)",
        "setup": "Pre-trained on CC3M and evaluated on multiple multilingual datasets to compare performance consistency across languages.",
        "hyperparameters": "Learning rate of 1e-4, batch size of 64, 30 epochs, using AdamW optimizer with decoupled weight decay.",
        "results": "Achieved superior Recall@K and lower MRV values in comparison to baseline methods across CCR datasets, demonstrating enhanced inter- and intra-modal consistency.",
        "performance": "Outperformed state-of-the-art methods, showing fewer inconsistencies across language retrieval performance.",
        "analysis": "Confirmed that the proposed method addresses main drawbacks of pairwise contrastive learning in multilingual scenarios.",
        "ablation study": "Removal of key components like 1-to-K contrastive learning reduced performance, confirming their significance in model accuracy and consistency."
    },
    "conclusion": {
        "summary": "The paper introduces an effective contrastive learning paradigm for consistency improvement in CCR, offering a viable solution to inter- and intra-modal inconsistency challenges previously observed.",
        "future work": "Explore the potential for broadening language inclusivity and optimizing training efficiency further."
    }
}