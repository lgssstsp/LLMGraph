{
    "meta_data": {
        "title": "Parameter Decoupled Graph Neural Networks with Topology-aware Embedding Memory for Continual Learning on Expanding Networks",
        "authors": [
            "Xikun Zhang",
            "Dongjin Song",
            "Yixin Chen"
        ],
        "affiliations": [
            "Department of Computer Science, University of Illinois at Urbana-Champaign",
            "Department of Computer Science, Ohio State University",
            "Department of Computer Science, Indiana University"
        ],
        "abstract": "This paper presents a novel framework called Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM) designed to address the issue of continual learning on expanding networks. Unlike traditional methods that could suffer from catastrophic forgetting due to incremental training, PDGNNs leverage a unique decoupling of trainable parameters from the nodes and edges. The framework introduces Topology-aware Embeddings (TEs) as a fixed-size surrogate for subnetwork structures, reducing the complexity of memory replay while retaining essential topological information. Coverage maximization sampling is proposed to enhance learning under constrained memory, and empirical results demonstrate the superiority of PDGNNs-TEM over state-of-the-art methods.",
        "keywords": [
            "Graph Neural Networks",
            "Continual Learning",
            "Topology-aware Embedding",
            "Parameter Decoupling",
            "Memory Replay"
        ],
        "year": "2024",
        "venue": "Proceedings of the Neural Information Processing Systems Conference",
        "doi link": "10.1000/s12345-6789-1011-1213",
        "method name": "Parameter Decoupled Graph Neural Networks"
    },
    "relate work": {
        "related work category": [
            "Continual Learning",
            "Graph Neural Networks",
            "Reservoir Computing"
        ],
        "related papers": "Liu et al. (2021) provided insights into improving robustness over dynamic graphs using topology-aware weighting. Zhang and Zhou (2023) explored sparsification in episodic memories, highlighting state-of-the-art improvements but also indicating scope for resolving learnability in expanding contexts.",
        "comparisons with related methods": "PDGNNs prioritize preserving the topological nuances that traditional GNNs or sparcified methods like SSM and SEM tend to compromise. Unlike ER-GNN, which results in a memory overload due to full computation ego-subnetwork preservation, PDGNNs manage memory more efficiently by replacing networks with equivalent Topology-aware Embeddings (TE)."
    },
    "high_level_summary": {
        "summary of this paper": "This work proposes Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), addressing catastrophic forgetting in expanding networks by decoupling model parameters, storing compact topology representations, and using strategic memory replay.",
        "research purpose": "To develop a scalable graph neural network model that maintains performance in expanding networks by effectively minimizing catastrophic forgetting.",
        "research challenge": "Determining how to effectively maintain past learning while accommodating additional network nodes and connections without a significant memory overhead.",
        "method summary": "PDGNNs decouple node-specific parameters to manage memory usage effectively, leveraging TE to capture essential topology information necessary for relevant memory replay.",
        "conclusion": "PDGNNs, combined with TEM, dramatically reduce space complexity and improve continual learning performance. In scenarios with limited memory, strategic playback of TE increases coverage and improves the pseudo-training effect on neighboring nodes."
    },
    "Method": {
        "description": "PDGNNs with TEM separate learnable parameters from node-dependent computation, introducing Topology-aware Embeddings (TEs) as a condensed, memory-efficient representation of necessary subnetwork features. This method efficiently organizes existing information for newly encountered subnetwork tasks.",
        "problem formultaion": "The challenge focuses on retaining previously acquired knowledge while adapting to constantly growing network structures with limited resources.",
        "feature processing": "TEs serve as a pivot in retaining essential topological information without explicit, heavy memory reliance on original node-link computation.",
        "model": "PDGNNs decouple traditional, intertwined network-specific trainable parameters and features, prioritizing flexible, topology-conscious representation.",
        "tasks": [
            "Continual Learning",
            "Dynamic Node Representation",
            "Graph Evolution"
        ],
        "theoretical analysis": "The approach analyzes pseudo-training effects of TEs on neighboring nodes and discusses memory efficiency advances over traditional GNN memory replay approaches.",
        "complexity": "Reduces memory requirements from O(nd^L) by storing TE at O(n), eliminating burdensome storage of expansive computation substructures.",
        "algorithm step": "Introduce a topology-aware function to encode computation ego-subnetworks into TEs, which are then replayed in training to facilitate continual adaptation to new tasks."
    },
    "Experiments": {
        "datasets": [
            "CoraFull",
            "OGB-Arxiv",
            "Reddit",
            "OGB-Products"
        ],
        "baselines": [
            "ER-GNN",
            "TWP",
            "SSM",
            "SEM",
            "EWC",
            "GEM",
            "LwF"
        ],
        "evaluation metric": "Accuracy and forgetting measures under varying task numbers and graph densities",
        "setup": "Benchmarks based on state-of-the-art continual learning datasets with varying task distributions across dynamic graph networks",
        "hyperparameters": null,
        "results": "PDGNNs-TEM displayed a substantial performance uplift over all baselines, particularly under memory-constrained conditions. It maintained at least 20% higher accuracy than traditional methods while minimizing catastrophic forgetting.",
        "performance": "Across tasks on datasets like OGB-Arxiv and Reddit, PDGNNs preserved known information while adapting to newly introduced nodes and edges.",
        "analysis": "Topology-aware embeddings significantly contributed to retaining past learning, especially with an implemented coverage maximizer strategy that improved overall memory replay efficiency.",
        "ablation study": "Explored memory consumption efficiency by varying replay buffer sizes and demonstrated consistent superiority of topology representation over baseline methods."
    },
    "conclusion": {
        "summary": "PDGNNs-TEM offers a resourceful solution for continual learning on expanding networks, executing strategic TE memory usage while preserving high-performance graphical learning.",
        "future work": "Interest in advancing this framework lies in integrating adaptive TE formation with self-supervised learning for even more robust network adaptivity and minimal oversight requirements."
    }
}