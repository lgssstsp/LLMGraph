{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-collab",
    "Dataset Link": "../linkprop/#ogbl-collab",
    "Rank": 7,
    "Method": "PLNLP (val as input)",
    "External Data": "No",
    "Test Accuracy": "0.6872 ± 0.0052",
    "Validation Accuracy": "1.0000 ± 0.0000",
    "Contact": "mailto:wztzenk@gmail.com",
    "Paper Link": "https://arxiv.org/pdf/2112.02936.pdf",
    "Code Link": "https://github.com/zhitao-wang/PLNLP",
    "Parameters": "35,112,192",
    "Hardware": "Tesla-P40 (24G GPU)",
    "Date": "Dec 7, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-collab/PLNLP_(val_as_input).pdf",
    "Paper Summary": "The paper \"Pairwise Learning for Neural Link Prediction\" introduces a framework called **PLNLP**, which treats link prediction as a pairwise learning-to-rank problem. It consists of four key components, each essential for its functionality:\n\n1. **Neighborhood Encoder**: \n   - This component extracts expressive neighborhood information for the input node pairs. The framework is flexible in that it can utilize various graph neural networks (GNNs), including Graph Convolutional Networks (GCN) and GraphSAGE, or link prediction-specific architectures like SEAL and NANs. Two types of neighborhood encoders are proposed:\n     - **Node Neighborhood Encoder (NNE)**: This encoder processes each node in a sample separately, extracting features from their respective neighborhoods.\n     - **Edge-level Neighborhood Encoder (ENE)**: This encoder captures structural interactions between the neighborhoods of the node pairs, providing a hidden representation derived from the entire neighborhood subgraph for the given node pair.\n\n2. **Link Score Predictor**:\n   - After obtaining hidden representations from the neighborhood encoder, this component calculates the link scores for the input node pair. Various scoring functions can be employed, including:\n     - **Dot Product**: Suitable for undirected graphs, it computes the score via a dot operation on the hidden representations.\n     - **Bilinear Dot Product**: This is used for directed graphs, generalizing the dot product by incorporating a learnable matrix.\n     - **Multi-layer Perceptron (MLP)**: The MLP can be applied to both NNE- and ENE-derived hidden representations, utilizing different input forms (like Hadamard product for undirected graphs and concatenation for directed).\n\n3. **Pairwise Learning with Ranking Objective**:\n   - The framework adopts a ranking approach to fulfill the link prediction task, emphasizing the need for ranking positive pairs higher than negative pairs rather than performing a binary classification. The learning objective aligns with maximizing the Area Under the Curve (AUC) through:\n     - **Pairwise Ranking Objective**: Ensures positive scores are greater than negative scores, explicitly focusing on improving the ranking performance instead of using traditional loss functions like cross-entropy.\n\n4. **Negative Sampling**: \n   - To generate effective negative samples, the framework discusses different strategies depending on the problem, including:\n     - **Global Sampling**: Uniformly selects negative samples from the entire set of possible node pairs.\n     - **Local Sampling**: Samples negatives while considering the context of the positive node pairs.\n     - **Adversarial Sampling**: Employs generative models to craft challenging negative samples, enhancing the robustness of the model.\n     - **Negative Sample Sharing**: To improve efficiency, it allows sharing of negative samples using random permutations across multiple training pairs.\n\nOverall, the design of the PLNLP framework emphasizes flexibility and effectiveness, particularly through its pairwise learning approach and various component options catered to the specifics of link prediction tasks."
}