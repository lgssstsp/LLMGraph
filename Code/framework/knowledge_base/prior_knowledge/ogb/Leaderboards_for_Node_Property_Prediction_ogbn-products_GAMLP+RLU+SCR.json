{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 14,
    "Method": "GAMLP+RLU+SCR",
    "External Data": "No",
    "Test Accuracy": "0.8505 ± 0.0009",
    "Validation Accuracy": "0.9292 ± 0.0005",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "3,335,831",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GAMLP+RLU+SCR.pdf",
    "Paper Summary": "The paper presents the SCR (Consistency Regularization) framework to enhance the training of Graph Neural Networks (GNNs) in semi-supervised settings. The primary objective of SCR is to improve the generalization ability of GNNs while utilizing both labeled and unlabeled data. \n\n### Key Design Aspects of SCR Framework:\n\n1. **Consistency Regularization**:\n   - The SCR framework integrates two strategies of consistency regularization designed to manage the trade-off between labeled and unlabeled data.\n   - The first method minimizes disagreements in predictions among different perturbed versions of a GNN model. Variations in the model’s predictions are generated through techniques such as data augmentation and stochastic elements in the model architecture (e.g., dropout).\n\n2. **Mean-Teacher Paradigm**:\n   - The second method, referred to as SCR-m (mean-teacher consistency regularization), employs the mean-teacher framework. In this approach, a teacher model, whose parameters are updated as an exponential moving average (EMA) of the student model (the one being trained), provides pseudo-labels for unlabeled nodes.\n   - The predictions from the teacher model guide the training loss computation, ensuring that the model learns to align its predictions with those from the teacher.\n\n3. **Noisy Prediction Generation**:\n   - The SCR design incorporates methods for generating noisy predictions by performing multiple forward passes of the GNN with dropout applied. This strategy aims to create diverse outputs, which can be leveraged to enhance training stability.\n\n4. **Pseudo-Labeling**:\n   - For unlabeled data, the SCR framework generates pseudo-labels, which are defined in two ways:\n     - In SCR, the pseudo-label for an unlabeled node is the average of its noisy predictions.\n     - In SCR-m, the pseudo-label is derived from the teacher model’s output.\n\n5. **Loss Function**:\n   - The overall loss is composed of two components: \n     - A supervised loss calculated on labeled nodes.\n     - An unsupervised consistency loss evaluated on confident unlabeled nodes based on their pseudo-labels.\n\n6. **Confidence-based Masking**:\n   - This technique filters out predictions deemed to be low-confidence. Unlabeled nodes that do not meet a certain confidence threshold are not used in calculating the consistency loss, ensuring stronger training signals.\n\n7. **Training Algorithm**:\n   - The training process includes mixing labeled and unlabeled nodes and iterating over several epochs where predictions are generated, pseudo-labels are assigned, and the loss is minimized. The process ensures that the model leverages both types of data effectively.\n\n### Scalability and Flexibility:\n- SCR is designed to be scalable, capable of handling graphs with extensive nodes and edges due to its efficient methods that avoid the necessity of loading the entire graph into memory at each training step. It is flexible enough to be applied to various GNN architectures, accommodating different models as its backbone.\n\nThe SCR framework establishes a systematic approach for enhancing GNN training through noise-induced robustness and self-training methodologies, providing significant generalization improvements."
}