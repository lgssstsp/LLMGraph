{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-arxiv",
    "Dataset Link": "../nodeprop/#ogbn-arxiv",
    "Rank": 18,
    "Method": "AGDN+BoT+self-KD+C&S",
    "External Data": "No",
    "Test Accuracy": "0.7431 ± 0.0014",
    "Validation Accuracy": "0.7518 ± 0.0009",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2012.15024",
    "Code Link": "https://github.com/skepsun/adaptive_graph_diffusion_networks_with_hop-wise_attention",
    "Parameters": "1,513,294",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Jul 22, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-arxiv/AGDN+BoT+self-KD+C&S.pdf",
    "Paper Summary": "The paper introduces Adaptive Graph Diffusion Networks (AGDNs), focusing on enhancing Graph Neural Networks (GNNs) through an innovative model design. This section outlines the key aspects of AGDNs, including the architecture and methodology used for graph diffusion.\n\n### Model Design Aspects:\n\n1. **Graph Diffusion Networks (GDNs)**:\n    - GDNs replace traditional graph convolution operators in GNN layers with efficient graph diffusion operators.\n    - This approach allows for multi-hop information to be captured at each layer while reducing complexity and improving scalability.\n    - The diffusion depth \\( K \\) controls the range of information propagated through the network.\n\n2. **Multi-layer Graph Diffusion**:\n    - The model incorporates a multi-layer architecture where each layer computes multi-hop representations sequentially. This is done without the need to maintain a high-dimensional diffusion matrix.\n    - Each GDN layer is defined as:\n      - Initialization: \\( H^{\\tilde{(l,0)}} = H^{(l-1)} W^{(l)} \\)\n      - Iterative diffusion: \\( H^{\\tilde{(l,k)}} = A H^{\\tilde{(l,k-1)}} \\)\n      - Aggregation: \\( H^{(l)} = \\sum_{k=0}^K \\theta^{(l,k)} H^{\\tilde{(l,k)}} + H^{(l-1)}W^{(l),r} \\)\n\n3. **Adaptive Mechanisms**:\n   - The AGDNs introduce two novel mechanisms for weighting coefficients: Hop-wise Attention (HA) and Hop-wise Convolution (HC).\n   - These mechanisms are designed to adaptively learn the importance of each hop in the diffusion process.\n\n4. **Hop-wise Attention (HA)**:\n    - HA calculates a learnable query vector that helps derive the weighting matrix by attending to the significance of different node representations during diffusion.\n    - The attention scores are computed as:\n      \\[\n      \\theta_{HA}^{(k)} = \\frac{exp(σ(ω_{ik}))}{\\sum_{k=0}^{K} exp(σ(ω_{ik}))}\n      \\]\n    - This allows each node's importance to differ based on its context, promoting more adaptable information propagation.\n\n5. **Hop-wise Convolution (HC)**:\n    - HC defines a learnable weighting tensor specific to hop-wise and channel-wise computations.\n    - It uses convolutional strategies to aggregate representations, allowing for differentiated treatment of nodes based on their feature channels.\n    - This method simplifies and optimizes the diffusion operations without increasing complexity substantially.\n\n6. **Layer-wise Weighting Coefficients**:\n    - AGDNs can assign different weighting coefficients for different nodes and channels or layers, enhancing the flexibility of feature learning. \n    - The unified weighting tensor can be adapted for capturing multi-hop information efficiently.\n\n7. **Transition Matrix**:\n    - The paper proposes a pseudo-symmetric normalized adjacency matrix to establish a more supportive transition mechanism in AGDNs. \n    - This matrix accounts for both source and destination nodes' in-degrees, improving performance by leveraging structural insights from the graph.\n\n### Key Contributions:\n- By integrating multi-layer generalized graph diffusion with moderate complexity and efficient runtime, AGDNs present a significant advance in the structure of GNNs.\n- The introduction of HA and HC mechanisms allows AGDNs to adaptively capture multi-hop information, enhancing the overall expressiveness of the model while maintaining computational efficiency. \n\nThis refined model structure promises to address common issues faced by deep GNNs, such as overfitting and over-smoothing, while being computationally feasible on resource-constrained environments."
}