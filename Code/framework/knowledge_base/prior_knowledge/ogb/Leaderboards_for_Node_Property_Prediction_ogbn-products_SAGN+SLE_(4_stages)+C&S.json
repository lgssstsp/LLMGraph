{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 15,
    "Method": "SAGN+SLE (4 stages)+C&S",
    "External Data": "No",
    "Test Accuracy": "0.8485 ± 0.0010",
    "Validation Accuracy": "0.9302 ± 0.0003",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2104.09376",
    "Code Link": "https://github.com/skepsun/SAGN_with_SLE",
    "Parameters": "2,179,678",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Sep 21, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/SAGN+SLE_(4_stages)+C&S.pdf",
    "Paper Summary": "In the paper, the authors propose the **Scalable and Adaptive Graph Neural Networks (SAGN)** and a corresponding training approach called **Self-Label-Enhanced (SLE) training** to enhance semi-supervised learning tasks on large graphs.\n\n### Model Design Aspects\n\n1. **Architecture of SAGN**:\n   - **Attention Mechanism**: SAGN integrates an attention mechanism to replace the naive concatenation operation found in earlier scalable models like SIGN. This attention mechanism assigns different importance to each hop when aggregating neighborhood information, allowing SAGN to adaptively gather features from nodes at varying distances.\n   - **Multi-hop Encoders**: SAGN employs multiple MLP (Multi-Layer Perceptron) encoders to process node features across multiple hops (K-hops) to capture richer contextual information. Each encoded representation is weighted using attention matrices to form a combined node representation.\n   - **Residual Connections**: The model includes residual connections to retain information from the original node features, aiming to enhance expressiveness and overall performance.\n\n2. **Self-Label-Enhanced (SLE) Training**:\n   - **Two-Stage Training**: The training process is divided into stages. In the first stage, the model trains on the raw training set. In subsequent stages, it incorporates hard pseudolabels and previously predicted probabilities to enhance the training dataset.\n   - **Label Propagation**: The SLE training framework combines self-training with label propagation to improve label information flow through the graph. Hard pseudolabels are generated and propagated using a new scalable label model, which helps to integrate label information with node feature information more effectively.\n   - **Removal of Inner Random Masking**: The model eliminates inner random masking during training, allowing for a more straightforward incorporation of label information.\n\n3. **Scalable Label Model**:\n   - This model is designed to operate alongside any scalable base model, utilizing the last smoothed label vectors as input. The label propagation step generates the necessary inputs for this label model, which enhances the label information embedded within the training samples.\n\n### Key Operations\n- The attention weights are computed using a softmax function applied to combined representations of different hop features, enhancing interpretability and ensuring that relevant node features are prioritized during the learning process.\n- The model training includes label propagation steps that generate and refine label embeddings across multiple iterations, contributing to the goal of improving overall model accuracy and performance.\n\n### Complexities and Efficiency\n- SAGN maintains a complexity similar to that of previous scalable graph neural networks while introducing enhancements in expressiveness and interpretability through its structure-aware components.\n- The training runtime and memory costs are optimized, allowing efficient operation even on large graphs, by managing the sizes of encoded representations effectively.\n\nOverall, the proposed SAGN and SLE methods focus on improving the expressiveness and efficiency of scalable graph neural networks, making them capable of handling large-scale graph data for semi-supervised learning tasks."
}