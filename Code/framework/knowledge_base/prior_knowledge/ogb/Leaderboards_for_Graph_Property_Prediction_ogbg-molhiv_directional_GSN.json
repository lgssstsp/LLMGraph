{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-molhiv",
    "Dataset Link": "../graphprop/#ogbg-mol",
    "Rank": 14,
    "Method": "directional GSN",
    "External Data": "No",
    "Test Accuracy": "0.8039 ± 0.0090",
    "Validation Accuracy": "0.8473 ± 0.0096",
    "Contact": "mailto:g.bouritsas@imperial.ac.uk",
    "Paper Link": "https://arxiv.org/pdf/2006.09252.pdf",
    "Code Link": "https://github.com/gbouritsas/graph-substructure-networks",
    "Parameters": "114,211",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Jul 28, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-molhiv/directional_GSN.pdf",
    "Paper Summary": "The paper introduces **Graph Substructure Networks (GSNs)** to improve the expressivity of Graph Neural Networks (GNNs) by incorporating subgraph isomorphism counting. Here are the key design aspects of the model:\n\n### Model Architecture\n1. **Topologically-Aware Message Passing**: GSN modifies the traditional message-passing framework of GNNs by integrating structural information during aggregation. Each neighbor's contribution is weighted based on its structural relationship with the central node.\n\n2. **Structural Role Encoding**: The GSN architecture explicitly encodes structural roles by counting the occurrences of specific substructures (e.g., cycles, cliques) within the graph. This information generates vertex and edge structural features, allowing the model to distinguish between different nodes and edges based on their respective structural contexts.\n\n3. **Graph Substructure Layer**: Central to GSN are two variants:\n   - **GSN-v**: Utilizes vertex counts to update node representations.\n   - **GSN-e**: Operates on edge counts, integrating the edge features with the structural identifiers in the message-passing process.\n\n4. **Message Computation**:\n   - The incoming message for a node is computed as a function of its hidden state and the structural identifiers, allowing the model to capture richer topological information.\n   - Aggregate messages from neighboring nodes, incorporating their structural features to update the node's state:\n     \\[\n     h^{t+1}_v = UP^{t+1}(h^t_v, m^{t+1}_v)\n     \\]\n   where \\(m^{t+1}_v\\) is computed considering the structural identifiers.\n\n5. **Invariant to Isomorphism**: By focusing on vertex and edge invariants through substructure counting, GSN maintains permutation equivariance, enabling it to generalize across different graph topologies.\n\n### Structural Features\n1. **Substructure Collections**: The model utilizes predefined sets of small graphs which can dictate the model's inductive bias based on the specific characteristics of the task or domain.\n2. **One-Hot Encoding**: Structural identifiers for vertices and edges are typically one-hot encoded based on the counts of their appearance, ensuring that the model can easily integrate this information during the learning process.\n\n### Summary of GSN Functionalities\n- **Locality Retention**: Unlike higher-order methods that lose locality, GSN retains the attractive properties of GNNs while incorporating structural information.\n- **Universality**: If the chosen collection of substructures is sufficiently rich, GSN can distinguish all non-isomorphic graphs, effectively achieving universality.\n- **Flexibility in Architecture**: The GSN can be viewed as a modified version of an existing GNN architecture by simply replacing one layer with a GSN layer, allowing for enhanced performance with considerable ease of integration.\n\nOverall, the model emphasizes a flexible architecture that effectively integrates structural insights derived from the graph's topology, significantly enhancing GNN expressivity without sacrificing computational efficiency or locality."
}