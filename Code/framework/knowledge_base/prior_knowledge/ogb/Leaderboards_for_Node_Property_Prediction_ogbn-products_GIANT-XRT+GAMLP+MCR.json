{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 12,
    "Method": "GIANT-XRT+GAMLP+MCR",
    "External Data": "Yes",
    "Test Accuracy": "0.8591 ± 0.0008",
    "Validation Accuracy": "0.9402 ± 0.0004",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "2,144,151",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GIANT-XRT+GAMLP+MCR.pdf",
    "Paper Summary": "The paper presents the SCR (Consistency Regularization) framework, which focuses on enhancing the training of Graph Neural Networks (GNNs) through two primary strategies of consistency regularization designed for semi-supervised learning. Here's a detailed summary of the methods discussed in the article:\n\n### SCR Framework\nThe SCR framework aims to improve GNN training effectiveness by leveraging unlabeled data. The two key strategies of SCR are:\n\n1. **Disagreement Minimization (SCR)**:\n   - This approach reduces the discrepancies among predictions from various perturbed versions of a GNN. Different perturbed models are generated through techniques such as data augmentation or by incorporating randomness (e.g., dropout).\n   - The strategy ensures that when the input is slightly altered, the resulting predictions remain consistent, thereby adhering to the low-density separation assumption. This consistency helps in improving the generalization ability of the GNN model.\n\n2. **Mean Teacher Consistency Regularization (SCR-m)**:\n   - SCR-m employs a teacher-student model paradigm. In this setup, a teacher model generates predictions that serve as pseudo-labels for training the student model.\n   - The parameters of the teacher model are derived through an Exponential Moving Average (EMA) of the student model parameters, which helps in stabilizing the training process. The consistency loss in this method is calculated between the teacher’s predictions and the student’s predictions.\n   - Pseudo-labeling is more robust in SCR-m since it uses a teacher model that is continuously updated, thus providing more reliable targets for the student model to learn from.\n\n### Noisy Prediction Generation\nA crucial aspect of the SCR framework is the generation of noisy predictions:\n- The framework employs dropout during model training to generate diverse predictions. By executing multiple evaluations of the model with different dropout configurations, multiple noisy predictions are obtained for the same input.\n- These noisy predictions are averaged to create a pseudo-label for the associated unlabeled data points, enhancing the training signal from unlabeled data.\n\n### Pseudo-labeling\n- The SCR framework creates pseudo-labels for unlabeled nodes based on the predictions from the noisy viewpoints. Pseudo-labels serve as additional training signals for the GNN, enabling it to benefit from the unlabeled data.\n- Pseudolabels can be sharpened to reduce entropy, promoting high-confidence predictions.\n\n### Loss Function\nThe total loss function in the SCR framework comprises:\n- A supervised loss component that focuses on the labeled nodes, generally the Cross Entropy loss.\n- An unsupervised consistency loss component that leverages the pseudo-labeling for unlabeled nodes. This encourages predictions to align with the generated pseudo-labels while employing confidence-based masking to filter out unreliable predictions.\n\n### Final Notes\nThe SCR framework is flexible and can be applied to various GNN architectures, making it a general method for enhancing GNN performance across differing tasks and datasets. It is designed to be efficient, reducing the overhead associated with multi-stage self-training methods, thus facilitating quicker convergence during training."
}