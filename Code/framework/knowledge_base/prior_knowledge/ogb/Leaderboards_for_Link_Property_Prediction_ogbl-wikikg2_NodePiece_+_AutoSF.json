{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-wikikg2",
    "Dataset Link": "../linkprop/#ogbl-wikikg2",
    "Rank": 17,
    "Method": "NodePiece + AutoSF",
    "External Data": "No",
    "Test Accuracy": "0.5703 ± 0.0035",
    "Validation Accuracy": "0.5806 ± 0.0047",
    "Contact": "mailto:mikhail.galkin@mila.quebec",
    "Paper Link": "https://arxiv.org/abs/2106.12144",
    "Code Link": "https://github.com/migalkin/NodePiece/tree/main/ogb",
    "Parameters": "6,860,602",
    "Hardware": "Tesla V100 (32 GB)",
    "Date": "Jul 17, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-wikikg2/NodePiece_+_AutoSF.pdf",
    "Paper Summary": "The paper introduces NodePiece, a novel method for creating compositional and parameter-efficient representations of large knowledge graphs (KGs) that addresses the limitations of conventional representation learning strategies. The main contribution lies in how it constructs node embeddings through an anchor-based approach rather than maintaining a unique embedding for each entity.\n\n### Model Design Aspects of NodePiece\n\n1. **Anchor-Based Vocabulary Construction**:\n   - NodePiece forms a fixed-size vocabulary of node pieces derived from a selected set of anchor nodes and relation types in the graph. The vocabulary size is significantly smaller than the number of nodes (|V| << |N|).\n   - Anchors serve as compositional building blocks for forming embeddings of other nodes. The selection of anchors can be done using deterministic methods (like centrality measures) or random strategies based on empirical performance.\n\n2. **Node Tokenization**:\n   - Each node is tokenized into a hash consisting of the k-nearest anchors, their respective distances from the target node, and the immediate outgoing relation types. This approach allows the representation of unseen nodes using the same fixed vocabulary, essential for inductive learning.\n   - For a node \\( n \\), the hash is constructed as:\n     \\[\n     \\text{hash}(n) = \\{a_i\\}_{i=1}^{k}, \\{z_i\\}_{i=1}^{k}, \\{r_j\\}_{j=1}^{m}\n     \\]\n     where \\( a \\) are anchors, \\( z \\) are distances to anchors, and \\( r \\) represents relations.\n\n3. **Encoding**:\n   - A crucial aspect of NodePiece is the encoding function \\( \\text{enc}(n): \\text{hash}(n) \\rightarrow \\mathbb{R}^d \\) that maps these hashes to a d-dimensional embedding for each node. The encoding function can leverage different types of neural network architectures:\n     - **MLP (Multi-Layer Perceptron)**: Used for faster processing and scalability with graphs containing many edges.\n     - **Transformer**: Allows capturing complex relational patterns but requires more computational resources.\n   - The overall architecture uses a simple additive approach incorporating positional encodings based on distances to maintain the hash structure while feeding it into the encoder.\n\n4. **Inductive Learning Capability**:\n   - The design explicitly allows the model to generalize to unseen nodes by utilizing the anchor-based vocabulary, making it suitable for dynamic contexts where nodes may be added or removed frequently.\n   - The model can also encode new relationships efficiently without needing to retrain or recompute the embeddings of all other nodes.\n\n5. **Parameter Efficiency**:\n   - NodePiece reduces the number of parameters significantly by relying on a small fraction of nodes as anchors, often leading to models that require up to 70 times fewer parameters than traditional embedding approaches while still achieving competitive performance on various graph tasks.\n\n6. **Compositional Representation**:\n   - The embeddings produced can be seen as combinations of various anchors and relation types, allowing for a flexible representation that can adapt to the structure of different graphs without requiring extensive prior knowledge of all nodes.\n\nIn summary, NodePiece leverages a fixed-size vocabulary built from anchors and relation types, along with a hashing strategy for encoding nodes, to create a scalable and efficient framework for knowledge graph representation learning. The design facilitates both transductive and inductive learning, making it versatile for various real-world applications."
}