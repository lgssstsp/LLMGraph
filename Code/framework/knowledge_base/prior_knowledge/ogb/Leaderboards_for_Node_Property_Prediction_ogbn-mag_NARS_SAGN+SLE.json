{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-mag",
    "Dataset Link": "../nodeprop/#ogbn-mag",
    "Rank": 9,
    "Method": "NARS_SAGN+SLE",
    "External Data": "No",
    "Test Accuracy": "0.5440 ± 0.0015",
    "Validation Accuracy": "0.5591 ± 0.0017",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2104.09376",
    "Code Link": "https://github.com/skepsun/HSAGN_with_SLE",
    "Parameters": "3,846,330",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Jun 29, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-mag/NARS_SAGN+SLE.pdf",
    "Paper Summary": "The paper presents a new model called **Scalable and Adaptive Graph Neural Networks (SAGN)**, which enhances the capabilities of existing Graph Neural Networks (GNNs) by introducing a more sophisticated architecture and training methodology.\n\n### Model Design Aspects:\n\n1. **Architecture of SAGN**:\n   - **Inception-like Module**: SAGN builds upon the **Scalable Inception Graph Neural Network (SIGN)** framework but replaces the redundant concatenation operation with a learnable attention mechanism that is aware of the graph structure.\n   - **Adaptive Attention Mechanism**: This mechanism allows SAGN to assign varying importance to different hops of neighborhood information, making it more expressive in capturing the relationships among nodes across several graph layers.\n\n2. **Multi-hop Encoders**:\n   - SAGN employs multiple layers of Multi-Layer Perceptrons (MLPs) to process multi-hop node features. The encoder transforms the K-hop representations of nodes, gathering information from neighbors recursively to enrich node features.\n\n3. **Integration of Attention Weights**:\n   - For each hop, diagonal attention matrices are introduced to weight the encoded representations before aggregating them. This approach facilitates a more nuanced understanding of the node’s local context, rather than treating all neighbors uniformly.\n\n4. **Post-Encoder Mechanism**:\n   - SAGN includes a residual layer that adds the original node features to the integrated representation. This residual connection is essential for preserving the initial feature information while enabling deeper representation learning.\n\n5. **Self-Label-Enhanced (SLE) Training Approach**:\n   - This new training framework seamlessly integrates self-training and label propagation strategies. The training process is segmented into stages, enhancing the training set iteratively.\n   - At each stage, confident predictions (hard pseudolabels) from previous models are used to participate in label propagation, enriching the label information and improving the model’s learning process.\n   - The model begins with a raw training set, and as the stages progress, it incorporates more pseudo-labeled nodes to bolster training.\n\n6. **Label Propagation without Inner Random Masking**:\n   - To incorporate label information into SAGN, label propagation is conducted without using random masking across training, making the process more systematic and effective. This modified label propagation iteratively refines node embeddings and label contexts over multiple stages.\n\n### Summary:\nBy enhancing the attention mechanism and integrating self-training with systematic label propagation, SAGN is designed to improve expressiveness while maintaining scalability. Its architecture allows adaptive gathering of neighborhood information, ensuring that graph structures inform the model’s learning effectively. The SLE approach amplifies the model’s capability in semi-supervised learning by leveraging pseudo-labels in an organized manner, illustrating a significant evolution in scalable GNN design."
}