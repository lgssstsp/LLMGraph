{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-collab",
    "Dataset Link": "../linkprop/#ogbl-collab",
    "Rank": 17,
    "Method": "GraphSAGE  (val as input)",
    "External Data": "No",
    "Test Accuracy": "0.5463 ± 0.0112",
    "Validation Accuracy": "0.5688 ± 0.0077",
    "Contact": "mailto:matthias.fey@tu-dortmund.de",
    "Paper Link": "https://arxiv.org/abs/1706.02216",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/collab",
    "Parameters": "460,289",
    "Hardware": "GeForce RTX 2080 (11GB GPU)",
    "Date": "Oct 19, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-collab/GraphSAGE__(val_as_input).pdf",
    "Paper Summary": "The paper introduces **GraphSAGE** (SAmple and aggreGatE), an inductive framework for generating low-dimensional embeddings for nodes in large graphs, which allows for generalization to unseen nodes. Here are the key aspects related to the methods and model design:\n\n### Model Design Aspects\n\n1. **Inductive Learning Framework:**\n   - Unlike traditional methods that require all nodes to be present during training (transductive), GraphSAGE learns to generate embeddings for unseen nodes based on node features and their local neighborhood structures.\n\n2. **Aggregation Functions:**\n   - GraphSAGE incorporates various aggregator functions designed to capture information from a node's neighbors. Each aggregator can sample a fixed-size subset of neighbors, which maintains computational efficiency.\n   - Several architectures for aggregators are presented:\n     - **Mean Aggregator:** Computes the average of the feature vectors of the neighbors.\n     - **LSTM Aggregator:** Applies Long Short-Term Memory networks to aggregate information, enhancing expressive capability but not inherently permutation invariant.\n     - **Pooling Aggregator:** Utilizes a multi-layer perceptron followed by max pooling to aggregate neighbor information, ensuring both symmetry and expressiveness.\n\n3. **Sampling Mechanism:**\n   - The algorithm uses neighborhood sampling techniques to manage memory and runtime effectively. A fixed-size sample is taken from a node's neighborhood, which allows the framework to maintain a predictable computational footprint.\n\n4. **Embedding Generation:**\n   - The embedding generation occurs in multiple layers (denoted by depth K). At each layer, nodes aggregate feature information from their sampled neighbors and concatenate it with their own feature vector before passing it through a neural network layer for transformation.\n   - This iterative process allows the node embeddings to incorporate increasingly broader context from the graph as more layers are added.\n\n5. **Learning Objective:**\n   - GraphSAGE adopts an unsupervised loss function to train the aggregators and learn the parameters. The loss encourages representations of nearby nodes to be similar while ensuring that distinct nodes have different representations. This allows the model to learn the structure of the graph inductively.\n\n6. **Minibatch Processing:**\n   - An adaptation of GraphSAGE for stochastic gradient descent is presented, allowing for efficient minibatch training. This keeps the computational complexity manageable while training on large graphs.\n\n7. **Flexibility Without Task-Specific Supervision:**\n   - GraphSAGE can be trained in both an unsupervised setting (without task-specific labels) and a fully supervised manner, making it flexible for various downstream tasks.\n\n8. **Conceptual Relationship to Weisfeiler-Lehman Test:**\n   - The framework is inspired by the Weisfeiler-Lehman graph isomorphism test. By learning representations that focus on neighborhoods, GraphSAGE can effectively capture structural information.\n\nThese methodological aspects make GraphSAGE a versatile and efficient tool for generating node embeddings in large, evolving graphs while allowing for inductive capabilities that are crucial for practical applications."
}