{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 16,
    "Method": "SAGN+SLE (4 stages)",
    "External Data": "No",
    "Test Accuracy": "0.8468 ± 0.0012",
    "Validation Accuracy": "0.9309 ± 0.0007",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2104.09376",
    "Code Link": "https://github.com/skepsun/SAGN_with_SLE",
    "Parameters": "2,179,678",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Sep 21, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/SAGN+SLE_(4_stages).pdf",
    "Paper Summary": "### Methods and Model Design of Scalable and Adaptive Graph Neural Networks (SAGN) with Self-Label-Enhanced Training (SLE)\n\n#### 1. Model Overview\nThe paper introduces **Scalable and Adaptive Graph Neural Networks (SAGN)**, designed to improve the efficiency and expressiveness of Graph Neural Networks (GNNs) for large-scale graphs. This model incorporates several key architectural innovations and techniques to achieve better performance, especially in semi-supervised tasks.\n\n#### 2. Key Innovations in SAGN\n\n- **Decoupled Architecture**: SAGN separates graph convolutions and learnable transformations into preprocessing and classifier components, allowing for mini-batch training and efficient handling of large graphs.\n\n- **Multi-Hop Encoders**: SAGN uses multi-hop neighborhood aggregation to gather information from various distances (hops) within the graph. This is enabled via a series of Multi-Layer Perceptrons (MLPs), which encode node features from multiple hops into representations.\n\n- **Attention Mechanism**: A learnable attention mechanism replaces the redundant concatenation operations in existing models like SIGN. This mechanism assigns importance to different hops based on the local distribution of nodes, enhancing expressiveness and interpretability.\n\n- **Integrated Representation**: SAGN effectively aggregates multi-hop features into a single representation using diagonal attention matrices, allowing for a more nuanced integration of neighborhood information.\n\n- **Residual Connections**: A residual connection is added between the integrated representation and the original input features, facilitating gradient flow and improving model stability.\n\n#### 3. Training Methodology: Self-Label-Enhanced (SLE) Training\n\n- **Self-Labeling**: SLE combines self-training and label propagation methodologies. Initially, the model is trained using the raw training set, and subsequent stages iteratively enhance the training set with hard pseudolabels generated from the model's previous predictions.\n\n- **Label Propagation**: At each stage of training, label propagation is performed to refine the input for a scalable label model based on one-hot encoded label vectors. This avoids inner random masking, ensuring that label information is effectively integrated without significantly complicating the training process.\n\n- **Stage-wise Training Process**: The training proceeds in stages, enhancing the training set iteratively. After establishing a base from the raw data, the model incorporates high-confidence nodes classified in earlier stages to create a richer training dataset for subsequent iterations.\n\n#### 4. Model Components\n\n- **Preprocessing Layer**: The preprocessing layer computes k-hop smoothed node feature matrices, preparing the data for the attention mechanism and multi-hop encoders.\n\n- **Label Model**: The label model is integrated with SAGN, processing label embeddings generated through label propagation, which supports the posterior augmented training of the main GNN model.\n\n#### 5. Scalable Label Model\nThe label model acts as a complementary module to the base GNN, incorporating high-confidence pseudolabels at each training stage while maintaining a separate processing pipeline for node features and label information.\n\nThe overall design of SAGN focuses on maximizing expressiveness through complex and adaptive mechanisms while maintaining scalability for large graph datasets. The attention mechanism and self-label-enhanced training synergistically address the challenges of efficiently training GNNs on extensive graphs."
}