{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-molhiv",
    "Dataset Link": "../graphprop/#ogbg-mol",
    "Rank": 18,
    "Method": "PNA",
    "External Data": "No",
    "Test Accuracy": "0.7905 ± 0.0132",
    "Validation Accuracy": "0.8519 ± 0.0099",
    "Contact": "mailto:gc579@cam.ac.uk",
    "Paper Link": "https://arxiv.org/abs/2004.05718",
    "Code Link": "https://github.com/lukecavabarrett/pna",
    "Parameters": "326,081",
    "Hardware": "NVIDIA Tesla T4 (15GB GPU)",
    "Date": "Nov 25, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-molhiv/PNA.pdf",
    "Paper Summary": "The paper presents a novel model design for Graph Neural Networks (GNNs), centered around the concept of **Principal Neighbourhood Aggregation (PNA)**. This model aims to enhance the expressive power of GNNs, particularly when handling continuous features. Here are the key methods and architectural aspects discussed:\n\n### 1. **Multiple Aggregators**\nThe authors argue for the necessity of using multiple aggregators to effectively extract information from the neighbouring nodes in a GNN. They prove that to accurately discriminate among multisets of size \\( n \\), at least \\( n \\) independent aggregators are needed. This allows the model to gain a richer representation of the graph structure.\n\n### 2. **Degree-Scalers**\nTo improve aggregation, the concept of **degree-scalers** is introduced. These are functions that adjust the aggregated messages based on the node degree, allowing signals to be amplified or attenuated according to the local graph structure. The use of scalers is proposed as a generalization of the traditional sum aggregation method. A logarithmic scaling function is suggested to mitigate the issue of exponential amplification in higher-degree nodes.\n\n### 3. **Normalization and Moments**\nThe authors also introduce higher-order moment aggregators, which allow extraction of nuanced distribution information from the signals. They focus on normalized moments (mean, standard deviation, etc.) to enhance understanding of the neighbourhood characteristics.\n\n### 4. **Architecture Design**\nThe PNA model employs a **message-passing architecture**, integrating multiple aggregation methods and scalers. The architecture allows for a flexible number of layers with shared parameters. It is structured as follows:\n\n- **Input Layer**: \nIncludes features from the nodes.\n  \n- **Aggregation Layers**: \nUtilize multiple chosen aggregation functions (mean, max, min, standard deviation). \n   \n- **Scaler Layers**: \nApply degree-scalers to the input of the aggregation functions to adjust the contributions based on the node degree.\n\n- **Update Function**:\nA neural network, typically implemented as a multi-layer perceptron (MLP), processes the aggregated messages to update the node features.\n\n### 5. **Combined Aggregation Method**\nThe PNA combines multiple types of aggregators and scalers into a single operation. For instance, it can utilize the mean, max, min, and standard deviation aggregators, each scaled appropriately.\n\n### 6. **Architecture Specifics**\nThe architecture features:\n- An encode-process-decode framework.\n- A variable number of convolution layers determined by the graph size, allowing the model to adapt to different graph complexities.\n- Weight sharing across layers to enhance parameter efficiency.\n\nIn summary, the PNA framework is designed to maximize the expressive capabilities of GNNs by employing a combination of multiple aggregators, degree-scalers, and effective architectural design principles. This advanced structural composition aims to improve the model's capacity in understanding and processing graph data."
}