{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 17,
    "Method": "SGC",
    "External Data": "No",
    "Test Accuracy": "0.6329 ± 0.0019",
    "Validation Accuracy": "0.6648 ± 0.0020",
    "Contact": "mailto:weihuahu@cs.stanford.edu",
    "Paper Link": "https://arxiv.org/abs/1902.07153",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/nodeproppred/papers100M",
    "Parameters": "144,044",
    "Hardware": "Xeon E7-8890x (1.5TB CPU)",
    "Date": "Jun 10, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/SGC.pdf",
    "Paper Summary": "The paper \"Simplifying Graph Convolutional Networks\" introduces the Simple Graph Convolution (SGC) model as a reduction of the complexity inherent in traditional Graph Convolutional Networks (GCNs). Here are the key methods and model design aspects discussed in the article:\n\n### Model Design Aspects\n\n1. **Linear Model Derivation**:\n   - The authors aim to derive a simplified linear model that could have preceded GCNs, motivated by the historical trend towards unnecessary complexity in deep learning architectures. The SGC reduces GCN complexity by removing nonlinear activation functions and simplifying the structure of the model.\n\n2. **Feature Propagation**:\n   - In contrast to GCNs, which apply multiple layers with nonlinearities, SGC operates by transforming the feature representations through a single step of feature propagation followed by a logistic regression classifier. \n   - The propagation is achieved using a normalized adjacency matrix \\( S = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\) with added self-loops, allowing for a fixed filtering operation to smooth the features across the graph.\n\n3. **Reduction of Nonlinearities**:\n   - The authors eliminate the nonlinear activation functions between layers of the network. The resulting model emphasizes the problematic nature of using deep learning components when a linear relationship suffices for many applications. The formal mathematical relationship expresses the node representation after \\( K \\) steps of propagation as:\n     \\[\n     H^{(k)} = S^K H^{(0)}\n     \\]\n   - Thus, the model's entire complexity is condensed, focusing instead on the simplification of representing node features via linear transformations.\n\n4. **Classifier Structure**:\n   - The SGC uses a softmax classifier to predict labels after feature propagation without additional layers of transformations. This maintains an intuitive interpretation where the feature extraction step is free of parameters, simplifying the training process.\n\n5. **Understanding Through Spectral Analysis**:\n   - The authors provide theoretical backing by linking SGC to the concept of spectral filtering on graphs, identifying it as a low-pass filter. This approaches graph convolution by defining the propagative action in terms of polynomials of the graph Laplacian, fundamentally establishing that while GCNs use first-order Chebyshev polynomials, SGC can be represented as a single fixed filter.\n\n6. **Fixed Parameterization**:\n   - SGC significantly reduces the number of parameters by consolidating transformations into a single weight matrix, which is optimized directly within the logistic regression framework. This aligns with the intuition that many machine learning tasks can be efficiently solved with simpler linear classifiers when the data structure is sufficiently represented.\n\nIn summary, the SGC aims to simplify the structure of graph convolutional networks without losing predictive power by focusing on linearity and interpretability, minimizing computational overhead through a single propagation mechanism followed by logistic regression."
}