{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-mag",
    "Dataset Link": "../nodeprop/#ogbn-mag",
    "Rank": 8,
    "Method": "NARS-GAMLP+SCR-m",
    "External Data": "No",
    "Test Accuracy": "0.5451 ± 0.0019",
    "Validation Accuracy": "0.5590 ± 0.0028",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/SCR",
    "Parameters": "6,734,882",
    "Hardware": "GeForce RTX 3090 24GB (GPU)",
    "Date": "Jun 13, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-mag/NARS-GAMLP+SCR-m.pdf",
    "Paper Summary": "The paper introduces the SCR (Consistency Regularization) framework aimed at enhancing the training of Graph Neural Networks (GNNs) in semi-supervised settings. The core design revolves around employing consistency regularization strategies to effectively utilize both labeled and unlabeled data.\n\n### Key Methods in SCR Framework:\n\n1. **Consistency Regularization**:\n   - **SCR**: This strategy minimizes the disagreement among perturbed predictions generated by different versions of a GNN model. Perturbations can occur through data augmentation or stochastic elements within the model, like dropout. By encouraging predictions from these variations to be aligned, the framework enhances the generalization ability of the GNN.\n\n2. **Mean Teacher Consistency Regularization (SCR-m)**:\n   - This variant utilizes a teacher-student model approach. In SCR-m, a consistency loss is calculated between the predictions of the student model and the averaged predictions from the teacher model, which is derived from the Exponential Moving Average (EMA) of the student’s parameters. This method stabilizes the predictions by leveraging the slowly evolving teacher model, thus providing more reliable pseudo-labels for the unlabeled nodes.\n\n3. **Noisy Prediction Generation**:\n   - The framework generates noise during training to promote regularization. Only dropout is employed to create multiple evaluations of the same input, leading to diverse predictions while maintaining efficiency, especially on large graphs. \n\n4. **Pseudo Labeling**:\n   - For unlabelled nodes, pseudo labels are constructed. In SCR, this is achieved via averaging the predictions from multiple noisy evaluations, while in SCR-m, it adopts the predictions from the EMA teacher model. A sharpening function is applied to reduce the entropy of the generated pseudo-labels, ensuring the model learns to produce confident, low-entropy predictions.\n\n5. **Loss Function Design**:\n   - The total loss function is a combination of a supervised component (cross-entropy loss from labeled nodes) and an unsupervised component (the distance metric between pseudo labels and predictions from confident unlabeled nodes). The balance between these components is controlled by a hyperparameter lambda (λ).\n\n6. **Confidence-based Masking**:\n   - To ensure that only predictions with high confidence contribute to the consistency loss during training, a masking mechanism filters out low-quality pseudo labels based on a confidence threshold. This helps focus training on more reliable predictions, improving overall model robustness.\n\n7. **Warmup Strategy for SCR-m**:\n   - Initially, during the early phases of training SCR-m, the regularization effect is disabled to prevent the student model from being misled by the less stable teacher model. This allows the student network to fit the labeled data effectively first before the teacher's influence is introduced.\n\n### Scalability and Flexibility:\nThe SCR framework is designed to be scalable, capable of handling models on graphs with millions of nodes, and is flexible enough to be applied to various GNN architectures like SAGN, GAMLP, GraphSAGE, etc.\n\nIn summary, the SCR framework leverages consistency regularization through innovative strategies such as noisy predictions, teacher-student models, and confidence-based mechanisms to strengthen the training process in semi-supervised settings."
}