{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-arxiv",
    "Dataset Link": "../nodeprop/#ogbn-arxiv",
    "Rank": 6,
    "Method": "GLEM+RevGAT",
    "External Data": "Yes",
    "Test Accuracy": "0.7694 ± 0.0025",
    "Validation Accuracy": "0.7746 ± 0.0018",
    "Contact": "mailto:andy.zhaoja@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2210.14709",
    "Code Link": "https://github.com/AndyJZhao/GLEM",
    "Parameters": "140,469,624",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Oct 27, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-arxiv/GLEM+RevGAT.pdf",
    "Paper Summary": "The paper introduces an innovative method called Graph and Language Learning by Expectation Maximization (GLEM) designed for learning on text-attributed graphs (TAGs). The approach effectively integrates both large language models (LMs) and graph neural networks (GNNs) to enhance node representation learning, facilitating better scalability and performance. Below are the key model design aspects discussed in the article:\n\n### Model Framework\n- **Variational EM Framework:** GLEM leverages a pseudo-likelihood variational framework to alternate between optimizing LMs and GNNs through Expectation (E) and Maximization (M) steps. This separation allows for more efficient training compared to traditional methods that require simultaneous training of both architectures.\n\n### E-Step and M-Step\n1. **E-Step (LM Optimization):**\n   - The GNN is fixed, while the LM is optimized to predict node labels, leveraging both observed labels and pseudo-labels predicted by the GNN. \n   - The goal is to maximize the evidence lower bound, which involves minimizing the Kullback-Leibler (KL) divergence between the predicted labels and the true labels.\n   - Pseudo-labels generated by the GNN are incorporated into the training of the LM to enhance label prediction.\n\n2. **M-Step (GNN Optimization):**\n   - In this phase, the LM is fixed, and the GNN is optimized. \n   - The GNN learns node representations based on the text embeddings provided by the LM and pseudo-labels generated by the LM during the E-step.\n   - The GNN effectively utilizes both structural information from neighboring nodes and textual information from the LM to predict labels more accurately.\n\n### Modeling Distributions\n- **Distribution q:** Represents the node label distribution conditioned on local text attributes (node descriptions) and is modeled using a transformer-based LM. It assumes node labels are independent given their textual features.\n  \n- **Distribution p:** Represents the conditional distribution of node labels based on text attributes and neighboring node labels. This is characterized using a GNN, thus capturing structural interactions between nodes.\n\n### Knowledge Distillation\n- The model utilizes a process of knowledge distillation wherein each component shares information with the other through pseudo-labels, enhancing the learning of both components through iterative training.\n\n### Hyperparameters\n- Two critical hyperparameters $\\alpha$ and $\\beta$ are introduced to balance the contributions of observed labels and pseudo-labels in the training objectives for the LM and GNN, respectively. The model encourages the effective interplay of both components, ensuring they benefit from each other's learned knowledge.\n\n### Scalability and Efficiency\n- GLEM’s architecture is built to maintain scalability, accommodating large models and datasets without the prohibitive memory costs often associated with training LMs and GNNs together. This is achieved through the E-step and M-step approach, which facilitates independent yet interconnected learning.\n\nIn summary, GLEM is a sophisticated framework that adeptly combines LMs and GNNs through a well-structured variational inference approach, allowing for efficient and effective node representation learning on large-scale text-attributed graphs."
}