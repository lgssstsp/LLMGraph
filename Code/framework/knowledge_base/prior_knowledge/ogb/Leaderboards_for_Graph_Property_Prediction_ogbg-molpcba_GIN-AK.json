{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-molpcba",
    "Dataset Link": "../graphprop/#ogbg-mol",
    "Rank": 11,
    "Method": "GIN-AK",
    "External Data": "No",
    "Test Accuracy": "0.2930 ± 0.0044",
    "Validation Accuracy": "0.3047 ± 0.0007",
    "Contact": "mailto:lingxiao@cmu.edu",
    "Paper Link": "https://arxiv.org/pdf/2110.03753v1.pdf",
    "Code Link": "https://github.com/GNNAsKernel/GNNAsKernel",
    "Parameters": "3,081,029",
    "Hardware": "RTX 3090",
    "Date": "Oct 11, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-molpcba/GIN-AK.pdf",
    "Paper Summary": "The paper introduces a framework called **GNN-AK (GNN As Kernel)**, which aims to enhance the expressiveness of any Message Passing Neural Network (MPNN) by integrating local structures in graph representation learning. Here’s a summary of the key model design aspects of GNN-AK:\n\n1. **General Framework**: GNN-AK acts as a wrapper that extends the aggregation mechanism of MPNNs from simple star-pattern (where a node aggregates information from its immediate neighbors) to more complex subgraph patterns, specifically k-hop egonets or induced subgraphs. This allows the model to better capture local structural information.\n\n2. **Subgraph Induction**: The design operates by defining a method to extract subgraphs rooted at each node. For each node, a k-hop egonet is identified, allowing for the encoding of relationships not just with immediate neighbors but also among neighbors. This assists in overcoming the limitations of MPNNs that rely excessively on local star structures.\n\n3. **Embedding Computation**: The representation of each node in GNN-AK is derived from the encoding of its surrounding induced subgraph. A base GNN (often an MPNN for scalability) is employed to produce node embeddings based on these subgraph representations.\n\n4. **Three Types of Encodings**: GNN-AK uses three different methods to encode node representations:\n   - **Subgraph Encoding**: This captures the general representation of the subgraph rooted at the node.\n   - **Centroid Encoding**: This focuses on the root node of the subgraph itself and its embedded representation.\n   - **Context Encoding**: This aggregates embeddings from subgraphs of neighboring nodes to provide a broader context for each node.\n\n5. **Pooling Layer**: A pooling mechanism is integrated into GNN-AK to combine results from the various encoding methods into a final node representation. Common pooling methods such as sum or mean can be applied across the extracted embeddings.\n\n6. **Subgraph Drop**: This is a sampling strategy that randomly \"drops\" subgraphs during training to reduce memory overhead while preserving performance. It involves selecting a reduced number of subgraphs covering all nodes adequately (with redundancy factors ensuring coverage), which allows the model to operate more efficiently without compromising the richness of the learned embeddings.\n\n7. **Distinction Capability**: Theoretically, GNN-AK is shown to be strictly more powerful than the 1st and 2nd order Weisfeiler-Lehman tests while being at least as powerful as the 3rd order. This is achieved by using subgraph encodings and hashing strategies that enable the model to distinguish non-isomorphic graphs effectively.\n\n8. **Implementation Practicality**: The framework is designed to work with any existing GNN implementation, facilitating a plug-and-play approach, thereby increasing adoption within the graph neural network community.\n\nThrough the above methods, GNN-AK leverages the structure of graphs more effectively, allowing for improved expressiveness and performance compared to traditional GNN models, while remaining computationally feasible for larger datasets."
}