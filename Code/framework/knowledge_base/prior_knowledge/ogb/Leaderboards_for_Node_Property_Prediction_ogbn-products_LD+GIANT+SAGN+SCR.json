{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 2,
    "Method": "LD+GIANT+SAGN+SCR",
    "External Data": "Yes",
    "Test Accuracy": "0.8718 ± 0.0004",
    "Validation Accuracy": "0.9399 ± 0.0002",
    "Contact": "mailto:zhihaoshi@mail.ustc.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2309.14907",
    "Code Link": "https://github.com/MIRALab-USTC/LD",
    "Parameters": "110,636,896",
    "Hardware": "GeForce RTX 3090 (24GB GPU)",
    "Date": "Sep 27, 2023",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/LD+GIANT+SAGN+SCR.pdf",
    "Paper Summary": "**Summary of Model Design Aspects: Label Deconvolution for Node Representation Learning**\n\nThe paper introduces a novel method called **Label Deconvolution (LD)** aimed at enhancing node representation learning on large-scale attributed graphs, particularly addressing the learning bias that arises when using separate training for Node Encoders (NEs) and Graph Neural Networks (GNNs).\n\n### Key Model Design Aspects:\n\n1. **Joint vs. Separate Training**:\n   - Traditional approaches often involve joint training of NEs and GNNs, which can be computationally intensive and suffer from scalability issues. In contrast, LD leverages a **separate training framework** that optimizes NEs and GNNs independently to handle large-scale datasets effectively.\n\n2. **Addressing Learning Bias**:\n   - The primary challenge in separate training is the **learning bias** that is introduced because the separate training disregards the feature convolutions in GNNs during NE training. LD addresses this by proposing an **efficient label regularization technique** that integrates GNNs' structures back into the training of NEs.\n\n3. **Label Deconvolution Mechanism**:\n   - LD generates **inverse labels** for NEs using an approximate inverse mapping of GNNs. These inverse labels (denoted as \\( Y(\\gamma) \\)) aim to align the learning process of NEs with that of GNNs, helping to recover structural information lost in traditional separate training approaches.\n\n4. **Model Optimization**:\n   - The optimization includes two phases:\n     - **Training Phase of NEs**: The objective is to minimize a loss function that combines the generated inverse labels with the features extracted from the NEs.\n     - **Training Phase of GNNs**: GNNs are trained on features generated by NEs using the structural information encapsulated within the inverse labels.\n   - The LD framework's design allows parameters for GNNs and NEs to be optimized in a decoupled manner while still leveraging the structural information for improved feature extraction.\n\n5. **Scalability**:\n   - LD aims to decouple the memory and computationally intensive aspects of feature convolutions from GNNs by pre-processing these components, thereby enhancing scalability.\n   - It incorporates i-hop labels—representing labels from neighboring nodes—integrating both node attributes and structural supervision for improved feature learning.\n\n6. **Loss Function Design**:\n   - The loss function in LD is designed to encourage the output of NEs to match the inverse labels, reflecting the true node classifications as informed by the GNN's structure. This design ensures that the learned representations from NEs effectively capture the interdependencies formed by the graph structure.\n\n### Conclusion:\nThe LD method is a strategic advancement in node representation learning, providing a robust mechanism to integrate the strengths of both NEs and GNNs in a scalable manner. By addressing learning bias through innovative label generation and model optimization strategies, LD enhances the efficacy of learning on attributed graphs while maintaining efficiency in resource usage."
}