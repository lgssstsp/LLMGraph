{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 17,
    "Method": "GAMLP+MCR",
    "External Data": "No",
    "Test Accuracy": "0.8462 ± 0.0003",
    "Validation Accuracy": "0.9319 ± 0.0003",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "3,335,831",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GAMLP+MCR.pdf",
    "Paper Summary": "The paper presents the SCR framework designed to enhance the training of Graph Neural Networks (GNNs) through consistency regularization, specifically targeting semi-supervised learning settings. This framework addresses the challenges of utilizing both labeled and unlabeled data effectively during model training.\n\n### Key Methodological Aspects:\n\n1. **Consistency Regularization Strategies**:\n   - **Standard SCR**: This approach minimizes the disagreement among predictions obtained from various perturbed versions of the same GNN model. The perturbations can come from different data augmentations or random dropout operations within the model. By doing so, SCR aims to improve the model's generalization ability by enforcing consistency across these diverse predictions.\n   - **SCR-m (Mean-Teacher Consistency Regularization)**: This variant employs a teacher-student paradigm, where the student is the current GNN model and the teacher is an exponentially moving averaged version of the student model. Instead of minimizing prediction disagreement directly, SCR-m computes a consistency loss based on the predictions from the teacher network and the perturbed predictions from the student. The parameters of the teacher model are updated at each training step using weights from the student model without additional backpropagation.\n\n2. **Noisy Prediction Generation**:\n   - SCR leverages dropout as a source of noise to generate multiple predictions for the same input. The model outputs noisy predictions, which are assumed to be small perturbations of the true predictions according to the low-density separation assumption. Multiple evaluations under various dropout settings yield different outputs, which helps in regularizing the model.\n\n3. **Pseudo Labeling**:\n   - In order to utilize unlabeled data effectively, SCR generates pseudo labels for unlabeled nodes. There are two approaches within the framework:\n     - In SCR, the pseudo label for an unlabeled node is derived as the average of its noisy predictions.\n     - In SCR-m, the pseudo labels are derived from the teacher model’s predictions using its learned parameters.\n\n4. **Loss Function**:\n   - The total loss function in SCR is a combination of:\n     - Supervised loss (cross-entropy between true labels and predictions for labeled nodes).\n     - Unsupervised consistency loss (measured for confident predictions of unlabeled nodes against their pseudo labels).\n\n5. **Confidence-based Masking**:\n   - SCR employs a confidence-based masking strategy during training. It filters out predictions of low confidence to avoid the use of unreliable pseudo labels that could mislead the model training.\n\n6. **Updating Mechanisms**:\n   - The teacher model in the SCR-m framework is updated with a decay factor, allowing it to reflect historical parameter values of the student model gradually, thus stabilizing training.\n\n7. **Hyperparameter Optimization**:\n   - The framework includes hyperparameters such as the consistency weight and various thresholds involved in confidence masking, allowing the model to be finely tuned for different tasks.\n\nOverall, the SCR framework offers a flexible, efficient, and scalable approach, successfully leveraging both labeled and unlabeled data through innovative noise management and consistency training methodologies and can be applied to various GNN architectures, enhancing their performance in semi-supervised learning tasks."
}