{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-arxiv",
    "Dataset Link": "../nodeprop/#ogbn-arxiv",
    "Rank": 7,
    "Method": "GIANT-XRT+AGDN+BoT+self-KD",
    "External Data": "Yes",
    "Test Accuracy": "0.7637 ± 0.0011",
    "Validation Accuracy": "0.7719 ± 0.0008",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2012.15024",
    "Code Link": "https://github.com/skepsun/Adaptive-Graph-Diffusion-Networks",
    "Parameters": "1,309,760",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Sep 2, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-arxiv/GIANT-XRT+AGDN+BoT+self-KD.pdf",
    "Paper Summary": "The paper presents Adaptive Graph Diffusion Networks (AGDNs), a novel approach to improve upon traditional Graph Neural Networks (GNNs) by addressing the overfitting and over-smoothing issues encountered in deep GNN models. This new model design emphasizes using multi-layer generalized graph diffusion in different feature spaces with moderate complexity and runtime.\n\n### Key Model Design Aspects:\n\n1. **Graph Diffusion Operator**: \n   - AGDNs replace the standard graph convolution operator in GNNs with a graph diffusion operator. This operator performs implicit graph diffusion by calculating multi-hop representations iteratively rather than storing a high-dimensional explicit diffusion matrix. This enhances memory efficiency while maintaining effective information propagation across the graph.\n\n2. **Weighting Coefficients**:\n   - AGDNs introduce learnable weighting coefficients to combine multi-hop node representations. Instead of using fixed coefficients, AGDNs adaptively learn these weights through two mechanisms:\n     - **Hop-wise Attention (HA)**: This mechanism allows for adaptive weighting of multi-hop information based on the significance of each hop. The attention scores are computed using a learned query vector, enabling the model to emphasize the most relevant neighborhood information effectively.\n     - **Hop-wise Convolution (HC)**: This method implements learnable convolutional methods across different hops, allowing for a channel-wise adaptation of weights. HC focuses on integrating hop-wise representations while optimizing for different feature channels directly through a convolution kernel.\n\n3. **Framework for AGDNs**:\n   - The design of AGDNs involves stacking multiple AGDN layers to perform extensive multi-layer graph diffusion. Each AGDN layer can be expressed through a specific formulation that incorporates intermediate multi-hop representations, allowing for flexible and dynamic modeling:\n     - Each layer begins with the input node feature matrix and iteratively computes layered representations using the transition matrix, updating the output based on hop-specific weights learned through HA or HC mechanisms.\n\n4. **Reciprocal Integration with Existing Features**:\n   - AGDNs are designed to preserve the essential characteristics of existing GNN architectures while facilitating a more generalized form of graph diffusion. This includes capabilities for incorporating non-linear transformations if desired, yet emphasizing the reduction of transformations' complexity to alleviate computational costs.\n\n5. **Node-wise and Channel-wise Adaptation**:\n   - The AGDN framework is adaptable such that different nodes and feature channels can have distinct hop-weighting matrices, allowing for a nuanced approach to information aggregation and representation learning on graphs.\n\nOverall, AGDNs aim for a paradigm shift in GNN model design by leveraging adaptive graph diffusion techniques, which increase the model's capacity without the high complexity typically associated with deep architectures, making it particularly suitable for large-scale graph-based tasks."
}