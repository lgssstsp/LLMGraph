{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-arxiv",
    "Dataset Link": "../nodeprop/#ogbn-arxiv",
    "Rank": 4,
    "Method": "LD+REVGAT",
    "External Data": "Yes",
    "Test Accuracy": "0.7726 ± 0.0017",
    "Validation Accuracy": "0.7762 ± 0.0008",
    "Contact": "mailto:zhihaoshi@mail.ustc.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2309.14907",
    "Code Link": "https://github.com/MIRALab-USTC/LD",
    "Parameters": "140,438,868",
    "Hardware": "GeForce RTX 3090 (24GB GPU)",
    "Date": "Sep 27, 2023",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-arxiv/LD+REVGAT.pdf",
    "Paper Summary": "### Summary of Model Design Aspects in \"Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs Against Learning Bias\"\n\n**Overview:**\nThe paper presents an innovative technique, **Label Deconvolution (LD)**, aimed at addressing the learning bias in node representation learning on large-scale attributed graphs. This approach enables the effective integration of pre-trained models as node encoders (NEs) with graph neural networks (GNNs).\n\n**Key Components of Model Design:**\n\n1. **Node Encoder (NE):**\n   - Pre-trained models (such as ESM2 for protein sequences and BERT for text) serve as NEs.\n   - NEs extract high-dimensional node features from attributes associated with graph nodes (e.g., texts, protein sequences).\n\n2. **Graph Neural Network (GNN):**\n   - GNNs build on the output of NEs and incorporate both node features and graph structures to iteratively update node representations.\n   - The model utilizes a diffusion matrix that is constructed from the adjacency matrix to capture graph topological information.\n\n3. **Separate Training Framework:**\n   - Existing methods had explored separate training of NEs and GNNs due to scalability issues, resulting in significant bias since they neglected the effect of feature convolutions from GNNs.\n\n4. **Label Deconvolution Mechanism:**\n   - LD is essentially a regularization approach that approximates the inverse mapping of GNNs. \n   - During the training phase of NEs, LD preprocesses the memory-heavy and computationally expensive feature convolutions so they only need to be executed once.\n   - This results in the formulation of an objective function that resembles joint training, significantly reducing the associated learning bias.\n   \n5. **Inverse Mapping Utilization:**\n   - LD integrates GNNs into the training phase of NEs by utilizing inverse labels derived from the GNN mapping. This ensures that NEs are trained with graph structures in mind, leading to more accurate feature representations.\n   - The formulation aims to recover the effectiveness of joint training without its inherent scalability issues.\n\n6. **Spectral-based GNNs:**\n   - LD leverages spectral-based GNNs, which allow for a more expressive representation through polynomial spectral filtering techniques.\n   - A key observation is that node labels depend not only on attributes but also on graph structures. LD corrects for this by ensuring labels (particularly inverse labels generated via GNNs) are optimized for training NEs using the spectral framework.\n\n7. **Training Phases:**\n   - The model design includes distinct training phases for NEs and GNNs, allowing for efficient and scalable training processes. In LD’s design, loss functions are calculated in mini-batches that improve learning efficiency.\n\n**Conclusion:**\nThe model design approach taken in this paper, especially through the integration of LD, allows for effective feature extraction from attributed graphs while preserving important structural information. By establishing a novel connection through the inverse mapping of GNNs, the design reduces learning bias and leverages both node attributes and graph structure for improved node representation learning."
}