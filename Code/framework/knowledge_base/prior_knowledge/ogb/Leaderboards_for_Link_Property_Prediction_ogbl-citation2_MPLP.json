{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-citation2",
    "Dataset Link": "../linkprop/#ogbl-citation2",
    "Rank": 1,
    "Method": "MPLP",
    "External Data": "No",
    "Test Accuracy": "0.9072 ± 0.0012",
    "Validation Accuracy": "0.9074 ± 0.0011",
    "Contact": "mailto:kevindong1994@gmail.com",
    "Paper Link": "https://arxiv.org/pdf/2309.00976.pdf",
    "Code Link": "https://github.com/Barcavin/efficient-node-labelling",
    "Parameters": "749,757,283",
    "Hardware": "NVIDIA A100 GPU (80G RAM)",
    "Date": "Jan 16, 2024",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-citation2/MPLP.pdf",
    "Paper Summary": "The paper introduces a novel approach called the **Message Passing Link Predictor (MPLP)**, which leverages the principles of pure message passing in Message Passing Neural Networks (MPNNs) to effectively estimate common neighbors (CN) for link prediction tasks. Here are the key aspects of the model design discussed:\n\n### Model Design Aspects:\n\n1. **Pure Message Passing**:\n   - The MPLP model builds on the idea that pure message passing can effectively capture joint structural features crucial for link prediction. It utilizes quasi-orthogonal (QO) vectors to enable efficient representation of the graph’s link structures.\n\n2. **Node Representation**:\n   - Each node in the graph is represented using QO vectors, which are initialized from a high-dimensional space to ensure they maintain their quasi-orthogonality through transformations in the network. This property facilitates the capturing of structural features without the permutation invariance issue typically associated with MPNNs.\n\n3. **Structural Feature Estimation**:\n   - The MPLP model incorporates a strategic design to estimate the number of walks between nodes, thus allowing the model to approximate the link-level features (e.g., counting common neighbors). The structural features are derived through multiple iterations of message passing which effectively accumulate information about the neighborhood of a target node pair.\n\n4. **QO Vector Construction**:\n   - QO vectors are constructed using a probabilistic sampling strategy over vertices of a hypercube. This method provides efficiency over deterministic constructions and ensures that the QO property remains intact during the message-passing process.\n\n5. **Inner Product for Estimation**:\n   - The estimation of CN mainly relies on the inner product between QO vectors of neighboring nodes. The model guarantees that the inner product accurately reflects the number of common neighbors, optimizing the process of counting structural features efficiently.\n\n6. **Integration of Node Attributes**:\n   - The model also considers node attributes where applicable, combining these features with the structural representations for a more comprehensive understanding of the graph. This integration allows the model to leverage both types of information during training.\n\n7. **Normalization and One-Hot Hubs**:\n   - MPLP incorporates normalization and one-hot encoding techniques for high-degree nodes (hubs) to enhance the model's capacity to estimate link features while reducing variance. This ensures that key nodes are weighted effectively during predictions.\n\n8. **Yang Representation**:\n   - The model formulates the link representation as a concatenation of the inner product of node features (GNN outputs) and additional structural estimates derived from the distance counts, thus packaging relevant information in one representation that feeds into link prediction classifiers.\n\nThrough these design elements, MPLP establishes a framework that enhances the expressiveness of GNN models in capturing the vital structural features necessary for effective link prediction, addressing limitations faced by previous methods."
}