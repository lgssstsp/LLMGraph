{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 8,
    "Method": "GIANT-XRT+SAGN+MCR",
    "External Data": "Yes",
    "Test Accuracy": "0.8651 ± 0.0009",
    "Validation Accuracy": "0.9389 ± 0.0002",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "1,154,654",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GIANT-XRT+SAGN+MCR.pdf",
    "Paper Summary": "The paper introduces the SCR (Consistency Regularization) framework, aimed at improving the training of Graph Neural Networks (GNNs) in a semi-supervised setting. Two key strategies are outlined within the SCR framework:\n\n1. **Disagreement Minimization (SCR)**: This method minimizes the discrepancies among predictions generated from perturbed versions of the GNN model. Perturbations can occur through data augmentation techniques or inherent randomness in the model (e.g., dropout). By reducing the variation in predictions across different model instances, SCR enhances the model's generalization ability.\n\n2. **Mean-Teacher Consistency Regularization (SCR-m)**: This strategy employs a teacher-student model paradigm. Here, a teacher model, which is a time-averaged version of the student model, produces predictions that guide the training of the student. Rather than minimizing prediction disagreements directly, SCR-m computes a consistency loss based on the predictions from both models. The teacher's parameters are updated based on an Exponential Moving Average (EMA) of the student model's parameters.\n\n### Model Design Aspects:\n\n- **Graph Representation**: The framework operates on graph-structured data, represented by nodes (features) and edges (relationships).\n  \n- **Architecture Flexibility**: SCR can be integrated with various GNN architectures such as GCN, GraphSAGE, GAMLP, and others—a testament to its design flexibility.\n\n- **Noisy Prediction Generation**: SCR proposes a method for generating noisy predictions utilizing dropout to induce randomness. Each node is evaluated multiple times under different dropout settings, yielding S noisy predictions, which are used for constructing pseudolabels.\n\n- **Pseudolabeling**: In SCR, the pseudolabel for each unlabelled node is averaged from its noisy predictions. In SCR-m, the pseudolabel is generated by the EMA teacher model. A further sharpening function is applied to this pseudolabel to reduce its entropy, promoting higher-confidence predictions.\n\n- **Loss Function**: The total loss in the SCR framework is a combination of a supervised component from labeled nodes and an unsupervised consistency loss component from the pseudolabeled unlabelled nodes. Confidence-based masking is employed to filter out predictions with low confidence, ensuring only high-quality pseudolabels influence training.\n\n- **Training Efficiency**: The SCR framework avoids multi-stage training overhead common in approaches like self-training by generating pseudolabels in a single step, which significantly reduces training epochs required for convergence.\n\nThe proposed SCR framework leverages these methods to enhance the performance of GNNs effectively, demonstrating its scalability and efficiency in handling massive graphs with millions of nodes and edges."
}