{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-mag",
    "Dataset Link": "../nodeprop/#ogbn-mag",
    "Rank": 15,
    "Method": "R-HGNN",
    "External Data": "No",
    "Test Accuracy": "0.5204 ± 0.0026",
    "Validation Accuracy": "0.5361 ± 0.0022",
    "Contact": "mailto:yule@buaa.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2105.11122",
    "Code Link": "https://github.com/yule-BUAA/R-HGNN.git",
    "Parameters": "5,638,053",
    "Hardware": "NVIDIA Tesla T4 (15 GB)",
    "Date": "May 24, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-mag/R-HGNN.pdf",
    "Paper Summary": "The paper proposes a **Relation-aware Heterogeneous Graph Neural Network (R-HGNN)** aimed at improving node representation learning within heterogeneous graphs by emphasizing the role of various relations. Below are the key aspects of the model's design:\n\n### 1. **Framework Overview**\nThe R-HGNN consists of multiple components that collaboratively learn both node and relation representations from heterogeneous graphs, allowing for better performance in downstream tasks.\n\n### 2. **Components of R-HGNN**\n\n#### **2.1 Relation-specific Node Representation Learning**\n- The heterogeneous graph \\( G \\) is deconstructed into multiple relation-specific graphs based on relation types.\n- A dedicated graph convolution module is designed to learn unique node representations for each relation-specific graph separately.\n\n#### **2.2 Cross-relation Message Passing**\n- After obtaining relation-specific representations, a cross-relation message passing module facilitates the interaction of these representations.\n- This module enhances the information flow among different relation-specific representations, allowing nodes to share information across different relation contexts.\n\n#### **2.3 Relation Representation Learning**\n- This module aims to capture the semantics of different relations explicitly, learning relation representations in a layer-wise manner.\n- The relation representations guide the node representation learning process, making the updates of node representations more informed regarding the type of relations.\n\n#### **2.4 Semantic Fusing Module**\n- The final component aggregates relation-aware node representations into a compact final representation.\n- This aggregation considers the learned importance of different relation representations, effectively fusing the diverse node representations learned from different relations.\n\n### 3. **Key Design Mechanisms**\n\n- **Weighted Residual Connection:** Combines target node features with aggregated neighbor information using trainable weights, enhancing the model's ability to adaptively integrate different types of information.\n  \n- **Layered Propagation Mechanism:** Captures interdependencies between relations through iterative updates across layers, making the learned representations more cohesive and informative.\n\n- **Attention Mechanism:** Utilizes multi-head attention to discern the importance of different relationships dynamically, allowing the model to focus on the most relevant information in each layer.\n\n### 4. **End-to-End Learning**\nThe entire model is trainable in an end-to-end manner, meaning that it can optimize all components simultaneously. The training process incorporates both labeled and unlabeled nodes, catering to various learning scenarios (e.g., supervised and unsupervised).\n\n### 5. **Overall Contributions**\nThe design highlights the critical interdependencies between nodes and relations, collectively learning richer and more informative representations by integrating relational semantics into the representation learning process.\n\nIn summary, R-HGNN employs a series of interrelated components that focus on the unique characteristics of relations within heterogeneous graphs, integrating relation representation learning, message passing, and semantic aggregation to enhance node representation quality."
}