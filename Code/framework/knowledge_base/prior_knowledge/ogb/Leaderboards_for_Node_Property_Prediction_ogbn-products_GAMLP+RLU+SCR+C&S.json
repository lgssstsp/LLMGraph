{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 13,
    "Method": "GAMLP+RLU+SCR+C&S",
    "External Data": "No",
    "Test Accuracy": "0.8520 ± 0.0008",
    "Validation Accuracy": "0.9304 ± 0.0005",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "3,335,831",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GAMLP+RLU+SCR+C&S.pdf",
    "Paper Summary": "The paper introduces the SCR framework, which leverages consistency regularization to enhance the training of Graph Neural Networks (GNNs) in semi-supervised settings. The focus is on overcoming the challenge of balancing the error from labeled and unlabeled data by introducing two main strategies within the SCR framework.\n\n1. **SCR Strategy**: This strategy minimizes the disagreements among the predictions made by different versions of a GNN model, which could arise due to data augmentation or inherent randomness in the model. The goal is to improve the model's generalization capability by ensuring that small perturbations in input data do not lead to significant differences in predictions.\n\n2. **Mean-Teacher Consistency Regularization (SCR-m)**: This approach employs a teacher-student paradigm, where a teacher model generates pseudolabels for unlabeled nodes based on an exponentially moving average (EMA) of the student model's weights. The training involves calculating a consistency loss between the student model's predictions and those generated by the teacher model, rather than simply minimizing prediction disagreement, as in the SCR strategy.\n\n### Model Design Aspects:\n\n- **Noisy Prediction Generation**: Noisy predictions are generated using dropout, where multiple predictions are made for each node by temporarily disabling a portion of the model’s hidden units. This approach results in varied predictions for the same input, which reflects the stochastic nature of the network and is essential for applying consistency regularization.\n\n- **Pseudo Labeling**: \n  - In SCR, the pseudolabel for a node is defined as the average of its noisy predictions generated through multiple runs.\n  - In SCR-m, the pseudolabel is derived directly from the teacher model’s predictions.\n  \n- **Confidence-Based Masking**: To improve the reliability of pseudolabels, a confidence threshold is established. Only predictions that exceed this threshold are considered during training, effectively filtering out unconfident pseudolabels that could mislead the learning process.\n\n- **Loss Function**: The total loss function utilized in the SCR framework is a combination of supervised loss (applying cross-entropy between ground-truth labels and predictions for labeled nodes) and an unsupervised consistency loss (penalizing the distance between predictions for unlabeled nodes and their pseudolabels).\n\n- **Training Utility**: The authors also design specific techniques for efficient training, such as varying the confidence masking threshold over time to gradually incorporate more nodes into the consistency regularization process.\n\n- **Flexibility**: SCR exhibits the flexibility to be integrated with various GNN architectures, enhancing models such as GCN, GraphSAGE, and GAMLP.\n\nOverall, the SCR framework focuses on leveraging consistency among predictions from various models or noise injections to improve graph-based learning, making it adaptable and scalable for larger graphs."
}