{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-wikikg2",
    "Dataset Link": "../linkprop/#ogbl-wikikg2",
    "Rank": 14,
    "Method": "ComplEx-RP (50dim)",
    "External Data": "No",
    "Test Accuracy": "0.6392 ± 0.0045",
    "Validation Accuracy": "0.6561 ± 0.0070",
    "Contact": "mailto:yihong.chen@cs.ucl.ac.uk",
    "Paper Link": "https://openreview.net/pdf?id=Qa3uS3H7-Le",
    "Code Link": "https://github.com/facebookresearch/ssl-relation-prediction",
    "Parameters": "250,167,400",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Nov 23, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-wikikg2/ComplEx-RP_(50dim).pdf",
    "Paper Summary": "The paper \"Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations\" presents a novel approach to enhance Knowledge Base Completion (KBC) through a self-supervised learning technique that incorporates relation prediction within the training objective.\n\n### Model Design Aspects\n\n1. **Training Objective**:\n   - The proposed objective builds upon the existing 1-vs-All loss framework. The new objective includes not only the prediction of the subject and object of the triples but also a term for predicting the relation type.\n   - Formally, the modified training objective is expressed as:\n     \\[\n     \\argmax \\left[ \\log P(s | p, o) + \\log P(o | s, p) + \\lambda \\log P(p | s, o) \\right]\n     \\]\n     where \\( \\lambda \\) is a hyperparameter that balances the contributions of each term in the objective.\n\n2. **Integration into Existing Models**:\n   - The relation prediction objective can be easily integrated into various existing KBC models without significant computational overhead, allowing models to be trained with minimal additional complexity.\n   - Code examples provided in the paper illustrate how to incorporate relation prediction into standard implementations, showing its compatibility with existing architectures.\n\n3. **Auxiliary Training Task**:\n   - Relation prediction serves as an auxiliary task that helps the model learn more informative representations by encouraging it to differentiate between various relation types during training.\n   - This dual focus enhances the model's ability to score triples more effectively by reinforcing learning about the relational context between entities.\n\n4. **Models Focused On**:\n   - The paper mainly examines factorization-based models including ComplEx, DistMult, CP, and TuckER. These models utilize different scoring functions that evaluate the relationships between subjects, predicates, and objects.\n   - For example, ComplEx employs complex vector representations to capture nuanced relationships, while DistMult uses bilinear interactions between embeddings.\n\n5. **Scoring Functions**:\n   - Each model has a distinct scoring function, such as:\n     - **ComplEx**: \\(\\phi(s, p, o) = \\text{Re}\\left(\\langle s, p, o \\rangle\\right)\\), where \\(s, p, o\\) are the embedding representations of the subject, predicate, and object, respectively.\n     - **DistMult**: \\(\\phi(s, p, o) = \\langle s, p, o \\rangle\\), calculating a tri-linear product.\n\n6. **Hyperparameter Optimization**:\n   - The model design allows for tuning hyperparameters, including the embedding size for entities and relations, regularization strength, and learning rates. These parameters can significantly influence performance in downstream KBC tasks.\n\n### Summary\n\nThe paper introduces a self-supervised training paradigm that enriches existing KBC models by embedding relation prediction as an auxiliary task, thereby improving their ability to learn multi-relational graph representations. This approach not only enhances the accuracy of link prediction but also offers a straightforward method to implement in current KBC frameworks, making it applicable for a wide range of model architectures."
}