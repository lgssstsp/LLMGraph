{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-ppa",
    "Dataset Link": "../linkprop/#ogbl-ppa",
    "Rank": 12,
    "Method": "AGDN",
    "External Data": "No",
    "Test Accuracy": "0.4123 ± 0.0159",
    "Validation Accuracy": "0.4332 ± 0.0092",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2012.15024",
    "Code Link": "https://github.com/skepsun/Adaptive-Graph-Diffusion-Networks",
    "Parameters": "36,904,259",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Sep 2, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-ppa/AGDN.pdf",
    "Paper Summary": "The paper presents Adaptive Graph Diffusion Networks (AGDNs), designed to address challenges in Graph Neural Networks (GNNs), particularly the issues of overfitting and over-smoothing typically associated with deep GNNs. \n\n### Model Design Aspects:\n\n1. **Graph Diffusion Framework**:\n   - AGDNs introduce a framework where traditional graph convolution operations within GNNs are replaced with generalized graph diffusion methods. This is done to enhance model capacity while managing complexity and runtime effectively.\n\n2. **Adaptive Multi-Layer Diffusion**:\n   - The AGDN architecture consists of multiple layers that perform graph diffusion, allowing for the combination of information across different hops in the graph. This setup widens the receptive field while using fewer feature transformations, thus tackling over-smoothing.\n\n3. **Weighting Coefficients**: \n   - **Hop-wise Attention (HA)**:\n     - HA allows for the calculation of hop-wise and node-wise weighting coefficients, making the contribution of each node in a multi-hop context adaptive. It utilizes a learnable query vector to derive importance scores for each hop.\n   - **Hop-wise Convolution (HC)**:\n     - HC extends the parameters to be channel-wise as well, enabling different learnable coefficients per channel that adaptively adjust the influence of each hop in the diffusion process. \n\n4. **Layer Definition**:\n   - Each AGDN layer is formulated to process intermediate representations at varying diffusion depths. The outputs combine contributions from multiple hops without maintaining high-dimensional diffusion matrices, which is memory-efficient. \n   - The layer's output is denoted as:\n     \\[\n     H(l) = \\sum_{k=0}^{K} \\Theta(k) \\otimes \\tilde{H}(l,k) + H(l-1)W(l),r\n     \\]\n     where \\(\\Theta(k)\\) denotes the generalized weighting coefficients and \\(\\otimes\\) signifies element-wise multiplication.\n\n5. **Transition Matrix Variations**:\n   - The authors propose a pseudo-symmetric normalized GAT transition matrix in order to capture the influence of both source node out-degrees and destination node in-degrees. This balances the contributions from different nodes effectively.\n\n6. **Scalability and Complexity**:\n   - The design intends to keep the model complexity manageable while enhancing its capacity. The additional time complexity from aggregating multiple hops is kept to \\(O(KEd)\\), while space complexity is \\(O(KNd)\\), where \\(K\\) is the number of hops, \\(E\\) the number of edges, and \\(N\\) the number of nodes.\n\n7. **Residual Connections**:\n   - AGDNs employ residual connections to help alleviate the issues associated with deeper architectures, further improving training stability and performance by blending features from lower layers.\n\nThis composite model design showcases the ability of AGDNs to adaptively manage complexity in training while maintaining flexibility in representation learning through the use of generalized graph diffusion and learnable mechanisms for weight adjustment across different hops."
}