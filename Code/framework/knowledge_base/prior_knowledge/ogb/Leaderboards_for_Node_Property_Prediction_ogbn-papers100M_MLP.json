{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 19,
    "Method": "MLP",
    "External Data": "No",
    "Test Accuracy": "0.4724 ± 0.0031",
    "Validation Accuracy": "0.4960 ± 0.0029",
    "Contact": "mailto:weihuahu@cs.stanford.edu",
    "Paper Link": "https://arxiv.org/abs/2005.00687",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/nodeproppred/papers100M",
    "Parameters": "144,044",
    "Hardware": "Xeon E7-8890x (1.5TB CPU)",
    "Date": "Jun 10, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/MLP.pdf",
    "Paper Summary": "The paper \"Open Graph Benchmark: Datasets for Machine Learning on Graphs\" introduces the Open Graph Benchmark (OGB), a set of benchmark datasets aimed at advancing research in graph machine learning (ML). Central to this initiative is the design of diverse and large-scale datasets that facilitate scalable, robust, and reproducible experimentation across various domains and tasks.\n\n### Model Design Aspects:\n\n1. **Dataset Characteristics**:\n   - The OGB datasets are categorized broadly into three types of graph ML tasks: node property prediction, link property prediction, and graph property prediction. Each dataset represents different real-world scenarios and applications.\n   - Datasets are constructed to allow for scalability, with examples like \"small\" graphs having over 100,000 nodes or more than 1 million edges, and \"large\" graphs comprising hundreds of millions of nodes or billions of edges.\n\n2. **Unified Evaluation Protocol**:\n   - OGB datasets feature a standardized protocol for data splitting and evaluation, intended to mitigate the inconsistencies present in prior benchmarks. Realistic data splits based on domain-specific characteristics (e.g., time, species, molecular structure) are used, as opposed to conventional random splits, which can yield overly optimistic performance results.\n\n3. **Implementation of Graph Neural Networks (GNNs)**:\n   - Several common GNN architectures are discussed, including:\n     - **Graph Convolutional Network (GCN)**: This architecture utilizes layered message passing to aggregate information from neighbors in the graph.\n     - **GraphSAGE**: This model extends GCNs by enabling inductive learning, where it can operate on unseen nodes by employing neighborhood sampling.\n     - **Graph Isomorphism Network (GIN)**: GIN has been proposed for its expressive power, allowing it to distinguish between different graph structures more effectively.\n   - **Augmented Structures**: Some models incorporate virtual nodes or additional edge features to enhance learning from graph structures, aiming to improve the capturing of relationships and information propagation within the graph.\n\n4. **Design Variations**:\n   - The paper emphasizes the importance of tailoring the GNN designs to suit the characteristics of the datasets. For instance, using different types of node and edge features helps improve model performance on specific tasks.\n   - **Attention Mechanisms**: Attention mechanisms may also be integrated into the GNN designs to selectively focus on more informative parts of the graph, thus improving performance in complex cases.\n\n5. **Scalability and Memory Management**:\n   - Considerations regarding the scalability of GNNs are paramount due to the large sizes of the OGB datasets. Techniques for mini-batch training (e.g., Neighbor Sampling, Cluster-GCN, and GraphSAINT) are employed for efficiency. These methods facilitate memory-efficient training by processing subgraphs instead of the entire graph at once.\n\n6. **Flexible Architecture**:\n   - The OGB package is designed to be compatible with popular deep learning frameworks like PyTorch, enabling easy integration of various graph ML models. This design encourages the use of customized and novel architectures that can benefit from OGB's diverse datasets.\n\nOverall, the paper outlines a vision for the development of robust graph ML models, encouraging experimentation with innovative designs that leverage the rich structure and diversity of graph data. The emphasis is on creating scalable frameworks that can address real-world challenges effectively while pushing the boundaries of current research in the field."
}