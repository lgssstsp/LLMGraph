{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-molhiv",
    "Dataset Link": "../graphprop/#ogbg-mol",
    "Rank": 17,
    "Method": "PHC-GNN",
    "External Data": "No",
    "Test Accuracy": "0.7934 ± 0.0116",
    "Validation Accuracy": "0.8217 ± 0.0089",
    "Contact": "mailto:tuan.le2@bayer.com",
    "Paper Link": "https://arxiv.org/abs/2103.16584",
    "Code Link": "https://github.com/bayer-science-for-a-better-life/phc-gnn",
    "Parameters": "110,909",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Apr 14, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-molhiv/PHC-GNN.pdf",
    "Paper Summary": "The paper presents a novel framework called **Parameterized Hypercomplex Graph Neural Networks (PHC-GNNs)** aimed at enhancing graph representation learning.\n\n### Model Design Aspects:\n\n1. **Hypercomplex Algebras**:\n   - The PHC-GNN framework generalizes graph neural networks (GNNs) by utilizing hypercomplex algebras, which include complex and quaternion algebras. These algebras facilitate a weight-sharing mechanism that enhances the model’s expressivity while reducing the number of parameters.\n\n2. **Parameterized Hypercomplex Multiplication (PHM) Layer**:\n   - A key innovation in the model is the **PHM layer**, which allows the multiplication rules defining the algebra to be learned from the data during training. This layer is a standard affine transformation structured as:\n     \\[\n     y = \\text{PHM}(x) = Ux + b\n     \\]\n     where \\(U\\) is constructed as a block-matrix derived from the sum of Kronecker products.\n\n3. **Contribution Matrices Initialization**:\n   - The authors propose a specific initialization for the contribution matrices \\(C\\) in the PHM layer to ensure they have full rank and are linearly independent. An alternate initialization method employs random sampling from a uniform distribution, which promotes a denser matrix representation and enhances training performance.\n\n4. **Weight-Sharing Mechanism**:\n   - The hypercomplex product defined in the PHM allows the model to share parameters efficiently. This is achieved by reducing the number of parameters required in weight matrices, leading to a lower memory footprint without compromising performance.\n\n5. **Message Passing Layer**:\n   - The model constructs its message-passing mechanism based on graph isomorphism networks (GIN). Each node aggregates information from its neighbors, accounting for edge features as well. It supports various aggregation strategies (e.g., sum, mean, max), tailored for different graph structures.\n\n6. **Skip Connections**:\n   - A mechanism for adding skip connections is incorporated to enhance information flow across layers. This helps in preserving important information from earlier layers while constructing deeper representations.\n\n7. **Batch Normalization**:\n   - The model employs standard batch normalization tailored for hypercomplex inputs, helping to stabilize training by maintaining zero mean and unit variance.\n\n8. **Regularization Techniques**:\n   - The PHC-GNN includes various regularization strategies such as weight and sparsity regularization, targeting both the main weight matrices and contribution matrices in the PHM layer to address overfitting and ensure better generalization.\n\n9. **Graph Pooling**:\n   - A pooling mechanism is employed post message passing to generate the graph-level representation. This pooling uses soft attention weights assigned to each node in the final hidden layer, ensuring optimal information capture from the node embeddings.\n\nThis methodology encapsulates the key design aspects of the PHC-GNN, focusing on the innovative use of hypercomplex numbers to enhance the expressiveness and efficiency of graph representation learning."
}