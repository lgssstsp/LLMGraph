{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-ppa",
    "Dataset Link": "../linkprop/#ogbl-ppa",
    "Rank": 16,
    "Method": "Matrix Factorization",
    "External Data": "No",
    "Test Accuracy": "0.3229 ± 0.0094",
    "Validation Accuracy": "0.3228 ± 0.0428",
    "Contact": "mailto:matthias.fey@tu-dortmund.de",
    "Paper Link": "https://arxiv.org/abs/2005.00687",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/ppa",
    "Parameters": "147,662,849",
    "Hardware": "GeForce RTX 2080 (11GB GPU)",
    "Date": "May 1, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-ppa/Matrix_Factorization.pdf",
    "Paper Summary": "### Summary of Methods in the Open Graph Benchmark (OGB)\n\nThe Open Graph Benchmark (OGB) introduces several methodologies for effective machine learning (ML) on graphs, focusing on scalable, robust, and reproducible graph ML research. Below are key aspects related to model design:\n\n#### 1. **Dataset Characteristics and Splitting**\n- OGB datasets are categorized into three types of graph prediction tasks: node property prediction, link property prediction, and graph property prediction. Each dataset is designed to have realistic and challenging data partitions:\n  - **Node Property Prediction:** Datasets like **ogbn-products** and **ogbn-proteins** leverage meaningful splits based on characteristics such as sales rank or species, facilitating a real-world application scenario.\n  - **Link Property Prediction:** Datasets (e.g., **ogbl-ppa**, **ogbl-collab**) are built to measure relationships based on historical data or experimental evidence.\n  - **Graph Property Prediction:** Datasets like **ogbg-mol**, **ogbg-ppa**, and **ogbg-code2** explicitly use scaffold and project splits, maintaining structural integrity and preventing leakage, allowing for more generalizable results.\n\n#### 2. **Graph Neural Network Architectures**\n- The OGB methodologies integrate various graph neural network (GNN) architectures tailored to specific tasks:\n  - **Graph Convolutional Network (GCN):** A standard model used primarily for node classification tasks.\n  - **Graph Isomorphism Networks (GIN):** Employed for their superior expressiveness, allowing for differentiation between graph structures.\n  - **GraphSAGE:** Utilized for its ability to sample neighborhoods dynamically, fitting into the framework of large-scale graph data without overwhelming memory.\n  - **Advanced Techniques:** Techniques like **virtual nodes** improve the model's capacity to generalize by embedding additional nodes linked to all existent nodes in the graph.\n\n#### 3. **Feature Engineering**\n- Custom features derived from graph structures enhance model performance:\n  - For molecular datasets, **node features** encapsulate atomic properties like chemical bonds, charges, and chirality, which are crucial for accurate predictions.\n  - In datasets like **ogbg-code2**, more complex features involving programming semantics (e.g., AST characteristics) enable better understanding of code structure and functionality.\n\n#### 4. **Model Training Approaches**\n- Two prevalent training strategies are discussed:\n  - **Full-Batch Training Analyzes:** Models like GCN and GIN are assessed using complete graph data sets, but scalability issues arise with large graphs.\n  - **Mini-Batch Training Techniques:** Such as **Neighbor Sampling**, **ClusterGCN**, and **GRAPHSAINT**. These approaches efficiently manage large datasets by breaking them into manageable batches, enabling training on graphs that exceed typical memory limits. \n\n#### 5. **Integration of Advanced Mechanisms**\n- Models are enhanced with operations such as:\n  - **Message Passing:** Allow for the aggregation of features from neighboring nodes, improving context learning during model training.\n  - **Edge Dropout and Noise Injection:** To regularize training, these methods introduce perturbations that help GNNs generalize better to unseen data.\n\nIn conclusion, the OGB emphasizes a structured and well-thought-out approach to model design, which takes into consideration data characteristics, advanced neural network designs, comprehensive feature integration, and tailored training methodologies to address the unique challenges presented by graph-based datasets."
}