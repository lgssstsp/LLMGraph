{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-proteins",
    "Dataset Link": "../nodeprop/#ogbn-proteins",
    "Rank": 3,
    "Method": "AGDN",
    "External Data": "No",
    "Test Accuracy": "0.8865 ± 0.0013",
    "Validation Accuracy": "0.9418 ± 0.0005",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2012.15024",
    "Code Link": "https://github.com/skepsun/Adaptive-Graph-Diffusion-Networks",
    "Parameters": "8,605,486",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Sep 2, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-proteins/AGDN.pdf",
    "Paper Summary": "The paper proposes Adaptive Graph Diffusion Networks (AGDNs) to address challenges in Graph Neural Networks (GNNs), notably over-smoothing and overfitting, while managing complexity and runtime.\n\n### Model Design Aspects:\n\n1. **Graph Diffusion Networks (GDNs)**:\n   - **Transition from Convolution to Diffusion**: GDNs replace traditional graph convolution operators with graph diffusion operators in each layer, facilitating the propagation of multi-hop node information without explicitly computing high-dimensional power matrices, thus improving efficiency.\n   - **Multi-hop Representation**: GDNs compute multi-hop node representations iteratively, significantly expanding the receptive field while maintaining manageable complexity.\n\n2. **Adaptive Graph Diffusion Networks (AGDNs)**:\n   - **Generalization of GDNs**: AGDNs extend GDNs by implementing a more flexible and adaptive framework for graph diffusion. AGDNs utilize two mechanisms for weight matrices that vary with nodes or channels.\n   \n3. **Hop-wise Attention (HA)**:\n   - **Learnable Weighting Coefficients**: The HA mechanism introduces learnable attention-based weighting coefficients that are node-wise and hop-specific. The attention scores are computed based on the inner product of the current node's features and a learnable query vector.\n   - **Dynamic Weight Adjustment**: This allows for dynamic adjustment of the importance given to different hops based on data representation, effectively enhancing model flexibility.\n\n4. **Hop-wise Convolution (HC)**:\n   - **Kernel-Based Weighting**: This mechanism uses a learnable convolution kernel matrix to adaptively weight the multi-hop representations but maintains a structure that is consistent across different nodes.\n   - **Channel-Specific Convolution**: HC can incorporate different convolution kernels for different feature channels, allowing the model to leverage the diversity of node attributes effectively.\n\n5. **Transition Matrix Design**:\n   - **Pseudo-Symmetric Normalization**: AGDNs propose a pseudo-symmetric normalized transition matrix, which balances the incoming and outgoing degrees of nodes, motivating enhancements in representation learning and ensuring robust message passing.\n\n6. **Activation and Integration**:\n   - **Element-Wise Multiplication and Summation**: AGDN layers combine multi-hop representations through element-wise multiplication (with learned weights) followed by summation with residual connections that integrate previous layer outputs, allowing for the preservation of original features and aiding in gradient flow during training.\n\n7. **Scalable Architecture**:\n   - **Complexity Considerations**: The design maintains a focus on keeping computational complexity low by limiting the number of transformations and ensuring that learned representations remain memory-efficient while expanding depth through adaptability in hop weightings.\n\nThis innovative architecture showcases a significant departure from traditional GNN designs by integrating flexible weight adaptations and diffusion-based strategies, aiming for improved performance on graph-related tasks while managing complexity and runtime efficiently."
}