{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 1,
    "Method": "GLEM+GIANT+SAGN+SCR",
    "External Data": "Yes",
    "Test Accuracy": "0.8737 ± 0.0006",
    "Validation Accuracy": "0.9400 ± 0.0003",
    "Contact": "mailto:andy.zhaoja@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2210.14709",
    "Code Link": "https://github.com/AndyJZhao/GLEM",
    "Parameters": "139,792,525",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Oct 27, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GLEM+GIANT+SAGN+SCR.pdf",
    "Paper Summary": "The paper introduces a novel approach called Graph and Language Learning by Expectation Maximization (GLEM) to tackle the challenge of learning representations on text-attributed graphs (TAGs). Central to GLEM is the idea of combining large language models (LMs) and graph neural networks (GNNs) using a variational Expectation-Maximization (EM) framework. Here's a breakdown of the model design aspects discussed in the article:\n\n### Method Overview\n\n1. **Dual Components**:\n   - **Language Model (LM)**: Encodes the textual attributes associated with each node, capturing local semantic information.\n   - **Graph Neural Network (GNN)**: Models the structural interactions between nodes, leveraging both the text and label information of surrounding nodes to predict node labels.\n\n2. **Variational EM Framework**:\n   - The GLEM employs a variational EM strategy that alternates between:\n     - **E-step (Expectation step)**: Updates the LM by predicting labels (including pseudo-labels inferred by the GNN) conditioned on the node’s textual description.\n     - **M-step (Maximization step)**: Updates the GNN using node representations generated by the LM, while utilizing both the observed labels and GNN-predicted pseudo-labels.\n\n3. **Objective Functions**:\n   - The framework aims to maximize the log-likelihood of the observed node labels while utilizing a lower bound of the evidence.\n   - The model parameterizations incorporate:\n     - For the LM, a mean-field approximation to represent the label distribution as independent of other nodes but dependent on its text attributes.\n     - For the GNN, a conditional distribution is defined that factors in the graph structure and surrounding node attributes (text and labels).\n\n4. **Pseudo-Labels**:\n   - GLEM uses pseudo-labeling where the LM generates predictions for unlabeled nodes, which are then employed as targets in GNN updates, fostering collaboration between the two models.\n\n5. **Training Procedure**:\n   - **E-step**: The LM is tuned to mimic label distributions inferred by the GNN while also learning from actual labels of labeled nodes.\n   - **M-step**: The GNN is trained to capitalize on the textual embeddings provided by the LM and the inferred labels to optimize the prediction of node classifications.\n\n6. **Scalability**:\n   - The EM framework allows GLEM to adaptively scale while maintaining efficiency, avoiding the memory overhead associated with joint training of LMs and GNNs, making it feasible for large TAG datasets.\n\nOverall, GLEM synergistically connects LMs and GNNs through a structured training paradigm that prioritizes the interaction between textual and structural information, enabling effective representation learning in large text-attributed graphs."
}