{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-code2",
    "Dataset Link": "../graphprop/#ogbg-code2",
    "Rank": 14,
    "Method": "GIN",
    "External Data": "No",
    "Test Accuracy": "0.1495 ± 0.0023",
    "Validation Accuracy": "0.1376 ± 0.0016",
    "Contact": "mailto:weihuahu@cs.stanford.edu",
    "Paper Link": "https://arxiv.org/abs/1810.00826",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/graphproppred/code2",
    "Parameters": "12,390,715",
    "Hardware": "GeForce RTX 2080 (11GB GPU)",
    "Date": "Feb 24, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-code2/GIN.pdf",
    "Paper Summary": "The paper \"How Powerful Are Graph Neural Networks?\" provides a theoretical framework for analyzing the expressive power of Graph Neural Networks (GNNs), focusing on their ability to represent and distinguish different graph structures. Here, we summarize the key methodological aspects related to model design:\n\n### 1. **Theoretical Framework**\nThe authors connect GNNs to the Weisfeiler-Lehman (WL) graph isomorphism test, which distinguishes graph structures through an iterative neighborhood aggregation technique. This analogy serves as the foundation for the evaluation of GNNs’ representational power. \n\n### 2. **Aggregation Functions**\nGNNs employ a neighborhood aggregation scheme where each node updates its representation by aggregating features from neighboring nodes. The essence of a GNN's power lies in its aggregation functions. The paper shows that to achieve strong representational capacity, a GNN’s aggregation functions must be able to map different multisets of feature vectors to different representations. The authors categorize various aggregation functions like:\n\n- **Sum Aggregation**: Combines all neighbor features through addition, preserving all structural information and allowing the model to retain distinct representations for varying multisets.\n  \n- **Mean Aggregation**: Captures the distribution of features within the neighborhood but is less powerful because it is not injective—different multisets can yield the same mean.\n\n- **Max Pooling**: Also not injective, may fail to capture relationships among nodes as it reduces the multiset to the largest feature value and, consequently, can lose structural details.\n\nThe work identifies that mean and max pooling can lead to significant representational limitations because they can yield identical outputs for different input graphs.\n\n### 3. **Graph Isomorphism Network (GIN) Design**\nThe authors propose a specific architecture called the Graph Isomorphism Network (GIN), designed to achieve the strongest discriminative power among GNNs. Key characteristics of GIN include:\n\n- **Injective Aggregation**: GIN aggregates neighboring features in a way that is injective, ensuring that different multisets produce different outputs in the hidden representation. This injectiveness is pivotal as it allows GIN to distinguish between different graph structures effectively.\n\n- **Refined Node Updates**: GIN updates node representations through a combination of functions, effectively allowing the model to encode complex relationships within the graph structure.\n\n- **Graph-Level Readout**: The model employs a \"readout\" function designed to summarize the entire graph representation based on its node features. This readout function concatenates information from multiple layers to retain comprehensive structural information.\n\n### 4. **Conditions for Maximally Powerful GNNs**\nThe research posits conditions under which a GNN achieves the representational capability of the WL test:\n\n- **Injectiveness in Aggregation and Readout**: The aggregation function must be injective, and similarly, the final graph-level readout must preserve distinct representations of the input graphs.\n\n### 5. **Generalization of WL Test**\nGIN generalizes the WL test, allowing it to not only distinguish between non-isomorphic graphs but also to capture similarities among various graph structures. Thus, it achieves state-of-the-art representational capabilities while being theoretically grounded.\n\n### Summary\nIn conclusion, the paper presents a detailed theoretical perspective on the design of GNNs, emphasizing that the choice of aggregation functions, particularly injective ones, plays a crucial role in their ability to learn and represent graph structures effectively. The introduction of GIN showcases an innovative architecture that bridges graph representation learning with established theoretical principles from graph theory."
}