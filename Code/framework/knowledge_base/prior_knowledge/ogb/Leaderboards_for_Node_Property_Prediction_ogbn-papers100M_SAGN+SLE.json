{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 9,
    "Method": "SAGN+SLE",
    "External Data": "No",
    "Test Accuracy": "0.6800 ± 0.0015",
    "Validation Accuracy": "0.7131 ± 0.0010",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2104.09376",
    "Code Link": "https://github.com/skepsun/SAGN_with_SLE",
    "Parameters": "8,556,888",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Apr 19, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/SAGN+SLE.pdf",
    "Paper Summary": "The paper presents a model designed to improve the scalability and adaptability of Graph Neural Networks (GNNs) through a new architecture called Scalable and Adaptive Graph Neural Networks (SAGN) and a novel training approach dubbed Self-Label-Enhanced (SLE).\n\n### Model Design: SAGN\n\n1. **Architecture Overview**:\n   - SAGN builds on the concepts of previous scalable GNNs and integrates a structure-aware attention mechanism to enhance expressiveness while maintaining simplicity.\n   - It decouples graph convolutions and transform functions into preprocessing and classifier stages. \n\n2. **Neighborhood Information Gathering**:\n   - SAGN utilizes **multi-hop neighborhood features**, gathering information from different graph connectivity levels.\n   - The model is structured to adaptively aggregate this neighborhood information using a diagonal attention mechanism, allowing the importance of neighbor nodes to vary based on their locality and relevance to the target node.\n\n3. **Attention Mechanism**:\n   - Different encoded node features from multiple hops are combined through learned attention weights, which capture the varying significance of these features. The scaled attention values are calculated using a LeakyReLU activation followed by a softmax operation.\n   - This attention mechanism enhances model interpretability and allows the model to dynamically focus on more relevant information across the different hops.\n\n4. **Preprocessing and Classification Layers**:\n   - SAGN employs multiple layers of Multi-Layer Perceptrons (MLPs) for both the multi-hop encoder and the classification stage.\n   - A residual connection is used in the final layer to preserve the original node features, enabling better integration of learned representations.\n\n### Training Approach: SLE\n\n1. **Label Incorporation**:\n   - The SLE method does not rely on inner random masking techniques prevalent in traditional self-training but instead uses a scalable label model. This model processes label information in a way that enhances connectivity and knowledge transfer within the graph.\n\n2. **Multi-Stage Training**:\n   - The training process is divided into multiple stages, where each subsequent stage incorporates predictions from the previous model. A threshold-based selection is used to identify confident nodes whose predicted features and labels contribute to the training set for subsequent models.\n\n3. **Label Propagation**:\n   - The SLE approach integrates label propagation as part of the training process, which utilizes the hard pseudolabels generated from the model's predictions. This allows labeled and pseudolabeled information to be diffused throughout the graph.\n   - The label model takes advantage of propagated label embeddings across multiple hops, optimizing information flow and reducing label leakage.\n\nIn summary, the SAGN architecture, combined with its SLE training approach, emphasizes adaptability and expressiveness through attention mechanisms, efficient preprocessing, label incorporation, and iterative training through self-labeling and label propagation—all aimed at improving GNN scalability and performance on large graphs."
}