{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 12,
    "Method": "GraphSAGE_res_incep",
    "External Data": "No",
    "Test Accuracy": "0.6706 ± 0.0017",
    "Validation Accuracy": "0.7032 ± 0.0011",
    "Contact": "mailto:mengyang.nmy@alibaba-inc.com",
    "Paper Link": "https://arxiv.org/abs/1706.02216",
    "Code Link": "https://github.com/mengyangniu/ogbn-papers100m-sage",
    "Parameters": "5,755,172",
    "Hardware": "Tesla V100 (16GB)",
    "Date": "Feb 28, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/GraphSAGE_res_incep.pdf",
    "Paper Summary": "The paper presents GraphSAGE (SAmple and agGreate), a framework for inductive representation learning on large graphs. Here's a detailed summary of the model design aspects discussed in the article:\n\n### Model Overview\nGraphSAGE aims to generate low-dimensional embeddings for graph nodes, particularly for unseen nodes in dynamic graphs. Unlike traditional methods that require embeddings for all nodes during training, GraphSAGE combines node feature information with local neighborhood aggregation to create a generalizable embedding function.\n\n### Key Techniques\n\n1. **Neighborhood Aggregation**:\n   - Instead of learning a separate embedding for each node, GraphSAGE learns a function to aggregate information from a node's local neighborhood.\n   - The aggregation is performed over a fixed-size, uniformly sampled set of neighbors, allowing the model to maintain computational efficiency and scale to large graphs.\n\n2. **Layered Architecture**:\n   - The model operates in multiple layers, each corresponding to a different depth of neighborhood aggregation. For each layer, nodes aggregate the results from their neighbors and combine this with their previous layer representation through a concatenation operation.\n   - This architecture enables the model to progressively incorporate information from increasingly distant nodes in the graph.\n\n3. **Aggregator Functions**:\n   - GraphSAGE uses multiple types of aggregation functions to combine neighbor features:\n     - **Mean Aggregator**: Computes the element-wise mean of the neighbor feature vectors.\n     - **LSTM Aggregator**: Employs a Long Short-Term Memory (LSTM) network to aggregate information from neighbors, enhancing expressive capability.\n     - **Pooling Aggregator**: Utilizes max-pooling across transformed neighbor representations to capture distinct aspects of neighborhood information.\n   - These aggregators ensure that the process is symmetric (order invariant), which is crucial for learning representations of unordered sets.\n\n4. **Training Procedure**:\n   - The model can be trained in both unsupervised and supervised manners. In the unsupervised setting, a graph-based loss function encourages nearby nodes to have similar representations.\n   - For supervised training, task-specific loss functions can augment or replace the unsupervised loss.\n\n5. **Forward Propagation Algorithm**:\n   - A structured forward propagation algorithm is used to compute embeddings efficiently in batches.\n   - This involves iterating over the layers and aggregating neighbor representations at each depth before normalizing the final outputs to avoid overfitting and promote generalizability.\n\n6. **Efficiency Enhancements**:\n   - The design emphasizes efficiency through neighborhood sampling, preventing the need to operate on the full graph at every iteration, making it feasible for large-scale data scenarios.\n   - Additionally, the architecture is capable of learning on evolving graphs effectively, adapting to newly introduced nodes without retraining on the entire dataset.\n\nIn summary, GraphSAGE integrates neighborhood sampling with multiple aggregation functions to produce embeddings that are efficient, scalable, and capable of generalizing to unseen nodes. This modular design allows for flexibility in choosing aggregation strategies based on specific application requirements."
}