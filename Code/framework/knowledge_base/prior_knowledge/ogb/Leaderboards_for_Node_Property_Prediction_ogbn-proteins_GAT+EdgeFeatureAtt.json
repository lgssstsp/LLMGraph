{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-proteins",
    "Dataset Link": "../nodeprop/#ogbn-proteins",
    "Rank": 11,
    "Method": "GAT+EdgeFeatureAtt",
    "External Data": "No",
    "Test Accuracy": "0.8682 ± 0.0021",
    "Validation Accuracy": "0.9194 ± 0.0003",
    "Contact": "mailto:espylapiza@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2103.13355",
    "Code Link": "https://github.com/Espylapiza/dgl/tree/master/examples/pytorch/ogb/ogbn-proteins",
    "Parameters": "2,475,232",
    "Hardware": "p3.8xlarge (15GB GPU)",
    "Date": "Nov 6, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-proteins/GAT+EdgeFeatureAtt.pdf",
    "Paper Summary": "The paper \"Bag of Tricks for Node Classification with Graph Neural Networks\" presents several novel techniques aimed at improving node classification performance through enhancements to model design and specifics of label usage in Graph Neural Networks (GNNs).\n\n### Key Methods and Model Design Aspects:\n\n1. **Label Usage**: The paper introduces a method that allows for the integration of both features and labels during training and inference. Specifically, it proposes a label reuse strategy that incorporates predicted labels from previous iterations as inputs for subsequent training phases. This approach aims to enhance the use of ground-truth label information throughout the training process.\n\n2. **Robust Loss Function**: The authors propose a new, quasi-convex loss function rather than typical logistic or Savage losses. The proposed Logeloss is designed to be less sensitive to outliers, thereby improving robustness in model training. The use of a non-decreasing function (\\(\\rho\\)) provides a way to handle potential issues with outlier sensitivity typically related to convex loss functions.\n\n3. **Sampling Techniques**: The paper emphasizes the performance of sampling strategies for efficiently training GNNs. It suggests using layer-wise and neighbor-aware sampling to better represent node features, thus enhancing feature propagation through the architecture. Additionally, negative sampling strategies are recommended to augment model training.\n\n4. **Architectural Adjustments**: The authors explore modifications to traditional GAT (Graph Attention Network) and GCN (Graph Convolutional Network) models by introducing:\n   - A **symmetric normalized adjacency matrix** to improve message passing.\n   - Resilient connections to mitigate over-smoothing, allowing the model to maintain distinct node feature representations over many layers.\n   - Variants of GAT that exploit both node and edge features to enhance the message passing process.\n\n5. **Combination of Label and Feature Propagation**: The paper discusses methodologies that enable the combined propagation of labels and features concurrently. This simultaneous approach aims to leverage the strengths of both label propagation algorithms and GNN architectures without falling into the trap of trivial solutions.\n\n6. **Iterative Enhancement Procedure**: An iterative refinement approach is suggested, which feeds the model's predictions from prior iterations into the next round of training. This technique reinforces label confidence, thereby improving model output robustness.\n\n7. **Augmentation with Label Reuse**: The technique of recycling predicted soft labels for unlabeled nodes helps in continuously refining predictions through the training process, ensuring that labeling aids in minimizing classification loss effectively.\n\nThese methods collectively aim to enhance node classification accuracy significantly, demonstrating that careful manipulation of model architecture and training strategies can yield improvements that surpass architectural advancements alone."
}