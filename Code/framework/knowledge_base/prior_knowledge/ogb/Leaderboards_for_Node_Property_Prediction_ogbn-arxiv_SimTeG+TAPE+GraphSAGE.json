{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-arxiv",
    "Dataset Link": "../nodeprop/#ogbn-arxiv",
    "Rank": 3,
    "Method": "SimTeG+TAPE+GraphSAGE",
    "External Data": "Yes",
    "Test Accuracy": "0.7748 ± 0.0011",
    "Validation Accuracy": "0.7789 ± 0.0008",
    "Contact": "mailto:k.duan@u.nus.edu",
    "Paper Link": "https://arxiv.org/abs/2308.02565",
    "Code Link": "https://github.com/vermouthdky/SimTeG",
    "Parameters": "1,381,593,403",
    "Hardware": "4 * A100-XMS4 (40GB GPU)",
    "Date": "Aug 7, 2023",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-arxiv/SimTeG+TAPE+GraphSAGE.pdf",
    "Paper Summary": "The paper introduces SimTeG, a novel yet straightforward approach for learning representations in Textual Graphs (TGs). The key methodological components of SimTeG can be summarized as follows:\n\n### Method Overview:\n1. **Two-Stage Training Process**:\n   - **Stage 1: Language Model Fine-Tuning**:\n     - A parameter-efficient fine-tuning (PEFT) is applied to a pre-trained language model (LM) on a textual corpus associated with the TG, focusing on a specific downstream task using task-specific labels.\n     - This results in improved contextual representation of the text features, which leverages the semantics of the input text.\n\n   - **Stage 2: Node Representation Generation**:\n     - Node embeddings are generated by utilizing the last hidden states of the fine-tuned LM. Instead of applying a full fine-tuning process which might lead to overfitting due to the large parameters, PEFT helps to mitigate this issue and ensures a more generalizable output.\n\n2. **Graph Neural Network (GNN) Training**:\n   - A GNN model, such as GraphSAGE, is employed to take these node embeddings and further train them on the task at hand (either node classification or link prediction) using the graph structure. The inputs to the GNN include the adjacency matrix of the graph alongside the derived node embeddings.\n\n3. **Decoupled Architecture**:\n   - The design allows for flexibility, as any GNN can be utilized on top of the embeddings generated by any LM, promoting modularity and separation of concerns between the language feature extraction and graph structure learning.\n\n4. **Regularization through PEFT**:\n   - The use of PEFT is crucial in both LM training and subsequent GNN training. By downplaying overfitting and ensuring a well-regularized feature space, PEFT helps produce node representations that demonstrably enhance downstream GNN performance.\n\n5. **LM Selection**:\n   - The method emphasizes the choice of the language model: models pretrained for retrieval tasks are selected as backbones for SimTeG, reflecting their suitability for the specific tasks of textual graph representation.\n\nThe implementation details are designed to be straightforward and easily replicable, focusing on existing architectures and requiring no complex modifications for training. The simplicity of the approach leverages the strengths of fine-tuned language representations to significantly boost the performance of GNNs applied to textual graphs."
}