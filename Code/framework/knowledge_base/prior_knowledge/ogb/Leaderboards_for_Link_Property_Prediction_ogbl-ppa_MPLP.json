{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-ppa",
    "Dataset Link": "../linkprop/#ogbl-ppa",
    "Rank": 1,
    "Method": "MPLP",
    "External Data": "No",
    "Test Accuracy": "0.6524 ± 0.0150",
    "Validation Accuracy": "0.6685 ± 0.0073",
    "Contact": "mailto:kevindong1994@gmail.com",
    "Paper Link": "https://arxiv.org/pdf/2309.00976.pdf",
    "Code Link": "https://github.com/Barcavin/efficient-node-labelling",
    "Parameters": "147,794,531",
    "Hardware": "NVIDIA A100 GPU (80G RAM)",
    "Date": "Jan 16, 2024",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-ppa/MPLP.pdf",
    "Paper Summary": "The paper introduces a new approach to link prediction using a novel model called the **Message Passing Link Predictor (MPLP)**, which addresses the limitations of existing Message Passing Neural Networks (MPNNs) in capturing joint structural features essential for link prediction.\n\n### Key Aspects of Model Design:\n\n1. **Pure Message Passing Paradigm**:\n   - MPLP utilizes a **pure message passing framework** that focuses on capturing structural features without relying on node attributes, allowing it to approximate heuristics like Common Neighbor (CN).\n\n2. **Quasi-Orthogonal Vectors**:\n   - The model leverages **quasi-orthogonal vectors** for estimating link-level structural features. These vectors help in preserving orthogonality within the input space, which is crucial for accurate computation of inner products needed to estimate characteristics like CN.\n\n3. **Node Representation**:\n   - Each node's representation is initialized as a **high-dimensional vector**. One-hot encoding is applied primarily for high-degree nodes (hubs) to maintain effective representation and minimize variance. This representation is crucial for passing messages through the graph.\n\n4. **Message Passing Mechanism**:\n   - MPLP employs a **simple message-passing mechanism** where node representations are updated based on the aggregated information from their neighbors. This is expressed mathematically as: \n     \\[\n     h^{(l+1)}_u = \\sum_{v \\in N_u} h^{(l)}_v\n     \\]\n   - An inner product operation captures the connections between nodes, allowing for the estimation of CN and other structural features.\n\n5. **Structural Feature Estimation**:\n   - MPLP computes various structural features such as the number of common neighbors, which is counted directly using the inner product of node representations post message passing. The model also includes methods to estimate counts for longer paths, e.g., paths of length two or triangles, which enrich the representation used for link prediction.\n\n6. **Hubs Identification**:\n   - The model introduces a systematic method to identify **hubs** (nodes of high degree) and assigns them unique one-hot vectors, which enhances the model's ability to capture important structural relationships.\n\n7. **Shortcut Removal**:\n   - To prevent shortcut connections from misleading the estimation process, a strategy of **shortcut removal** is implemented. This ensures that the model accurately reflects the true graph structure during training.\n\n8. **Norm Rescaling**:\n   - The MPLP design employs **norm rescaling** for the node representations, allowing for a weighted count of structural features that better reflect the relationships in the graph.\n\n9. **Iteration of Message Passing**:\n   - The model accommodates multiple iterations of message passing to explore deeper structural relationships, adjusting node representations iteratively based on their degrees of connectivity.\n\n10. **Scalability Considerations**:\n    - MPLP is designed to maintain efficiency and scalability in processing, allowing it to work effectively with large-scale graphs. The computational operations required for structural estimations are optimized to ensure they remain manageable even as graph size increases.\n\nIn summary, MPLP presents a refined model for link prediction that emphasizes the importance of accurately capturing joint structural features while maintaining computational efficiency through methods such as quasi-orthogonality, effective node representation, and strategic use of message passing."
}