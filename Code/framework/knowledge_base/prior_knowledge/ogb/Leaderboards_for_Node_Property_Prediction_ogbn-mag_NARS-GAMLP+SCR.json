{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-mag",
    "Dataset Link": "../nodeprop/#ogbn-mag",
    "Rank": 10,
    "Method": "NARS-GAMLP+SCR",
    "External Data": "No",
    "Test Accuracy": "0.5432 ± 0.0018",
    "Validation Accuracy": "0.5654 ± 0.0021",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/SCR",
    "Parameters": "6,734,882",
    "Hardware": "GeForce RTX 3090 24GB (GPU)",
    "Date": "Jun 13, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-mag/NARS-GAMLP+SCR.pdf",
    "Paper Summary": "The paper presents the SCR (Consistency Regularization) framework designed to enhance the training of graph neural networks (GNNs) in a semi-supervised setting. It addresses the challenge of balancing labeled and unlabeled data during training by introducing two strategies for consistency regularization.\n\n### Model Design Aspects:\n\n1. **SCR Framework**:\n   - The SCR framework encourages predictions from a GNN model to be consistent across multiple perturbed versions of the input data. This is based on the principle that small perturbations should not lead to significant changes in model predictions.\n\n2. **Consistency Regularization Strategies**:\n   - **First Strategy (SCR)**: This method minimizes the disagreement among predictions from different versions of a GNN model. Variations can be generated through data augmentations or the inherent randomness in the model (e.g., through dropout). By averaging these predictions, the model can improve its generalization ability.\n  \n   - **Second Strategy (SCR-m)**: This approach leverages the Mean-Teacher paradigm. Here, a \"teacher\" model is derived from the \"student\" model's parameters using an Exponential Moving Average (EMA). Instead of directly minimizing disagreement among the predictions, SCR-m aims to ensure that the predictions from the student model align closely with those from the teacher model. As the teacher model refreshes its parameters during training, it provides more stable predictions.\n\n3. **Pseudolabeling**:\n   - In the SCR framework, unlabelled nodes are assigned pseudolabels to facilitate training. For the SCR method, the pseudolabel for a node is calculated as the average of its predictions across multiple noisy versions. In contrast, SCR-m obtains pseudolabels from the teacher model's predictions.\n\n4. **Confidence-based Masking**:\n   - To enhance training effectiveness, the paper introduces confidence-based masking. This technique filters out predictions that are deemed unreliable, focusing on confident pseudolabels during training.\n\n5. **Loss Function**:\n   - The total loss combines supervised loss (derived from labeled data) and an unsupervised consistency loss (derived from unlabeled data). The unsupervised loss penalizes deviations of pseudolabels from the model's predictions, thereby enforcing consistency in outputs over multiple iterations.\n\n6. **Scalability and Flexibility**:\n   - The SCR framework is designed to be scalable and can efficiently handle large graphs with millions of nodes and edges. Moreover, it is flexible and can be applied to various modern GNN architectures, enhancing their performance without needing extensive modifications.\n\n### Summary:\nThe SCR framework incorporates innovative consistency regularization strategies that improve the performance of GNNs in semi-supervised settings. By focusing on decreasing disagreement among predictions and utilizing a teacher-student dynamic, along with effective pseudolabeling strategies, SCR demonstrates a robust design suitable for diverse GNN architectures."
}