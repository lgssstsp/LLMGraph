{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-code2",
    "Dataset Link": "../graphprop/#ogbg-code2",
    "Rank": 10,
    "Method": "GAT",
    "External Data": "No",
    "Test Accuracy": "0.1569 ± 0.0010",
    "Validation Accuracy": "0.1442 ± 0.0017",
    "Contact": "mailto:luoyk@buaa.edu.cn",
    "Paper Link": "https://arxiv.org/abs/1710.10903",
    "Code Link": "https://github.com/LUOyk1999/ogb/tree/master/examples/graphproppred/code2",
    "Parameters": "11,030,210",
    "Hardware": "GeForce RTX 3090",
    "Date": "Jan 15, 2023",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-code2/GAT.pdf",
    "Paper Summary": "### Methods Overview of Graph Attention Networks (GATs)\n\n#### Introduction to GATs:\nGraph Attention Networks (GATs) introduce a novel neural network architecture specifically designed for graph-structured data through the use of masked self-attention layers. The principal innovation lies in the ability to assign different importance weights to different nodes in the neighborhood without relying on computationally expensive matrix operations or needing prior knowledge of the graph structure.\n\n#### Graph Attention Layer:\nThe building block of GATs is the Graph Attention Layer, which processes node features and generates new feature representations. \n\n1. **Input Representation**: \n   - Each node is represented with a feature vector \\( \\mathbf{h}_i \\) of dimension \\( F \\).\n   - The input to the attention layer consists of a set of node features \\( \\mathbf{h} = \\{ \\mathbf{h}_1, \\mathbf{h}_2, \\ldots, \\mathbf{h}_N \\} \\).\n\n2. **Self-Attention Mechanism**:\n   - A shared linear transformation \\( W \\) is applied to every node to produce transformed features.\n   - An attention coefficient \\( e_{ij} \\) is computed as:\n     \\[\n     e_{ij} = a(W \\mathbf{h}_i, W \\mathbf{h}_j)\n     \\]\n     where \\( a \\) is a shared attention mechanism parameterized by a weight vector.\n\n3. **Masked Attention**:\n   - Only the nodes within a specified neighborhood \\( N_i \\) of node \\( i \\) are considered, enabling the model to work with the local graph structure.\n   - The attention coefficients \\( e_{ij} \\) are normalized using the softmax function to produce the attention weights \\( \\alpha_{ij} \\):\n     \\[\n     \\alpha_{ij} = \\text{softmax}(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in N_i} \\exp(e_{ik})}\n     \\]\n\n4. **Feature Aggregation**:\n   - The final output feature vector for each node is computed as:\n     \\[\n     \\mathbf{h}'_i = \\sigma \\left( \\sum_{j \\in N_i} \\alpha_{ij} W \\mathbf{h}_j \\right)\n     \\]\n     where \\( \\sigma \\) is a non-linear activation function.\n\n5. **Multi-Head Attention**:\n   - GAT models utilize multi-head attention by employing \\( K \\) independent attention mechanisms. The outputs from multiple heads are concatenated to create richer feature representations:\n     \\[\n     \\mathbf{h}'_i = \\bigoplus_{k=1}^{K} \\sigma \\left( \\sum_{j \\in N_i} \\alpha_{ik} W_k \\mathbf{h}_j \\right)\n     \\]\n   - For the prediction layer, an averaging strategy is used instead of concatenation, particularly when applying the final non-linearity.\n\n#### General Characteristics:\n- **Efficiency**: The computations can be parallelized across nodes and edges, avoiding costly matrix operations and eigendecompositions.\n- **Scalability**: The attention mechanism allows the model to generalize to larger and unseen graphs during testing, making it suitable for inductive learning tasks.\n- **Adaptability**: The combined architecture can manage nodes of varying degrees by assigning flexible weights to different neighbors.\n\nGATs leverage the power of attention to effectively model relationships in graph data, making them competitive when compared to traditional approaches that suffered due to structural constraints. The method’s design emphasizes computational efficiency and adaptability, addressing key challenges faced by previous graph-based neural network models."
}