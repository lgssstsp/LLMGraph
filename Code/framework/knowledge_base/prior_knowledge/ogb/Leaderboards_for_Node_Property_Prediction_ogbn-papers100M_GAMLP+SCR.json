{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 7,
    "Method": "GAMLP+SCR",
    "External Data": "No",
    "Test Accuracy": "0.6814 ± 0.0008",
    "Validation Accuracy": "0.7190 ± 0.0007",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/SCR",
    "Parameters": "67,560,875",
    "Hardware": "GeForce RTX 3090 24GB (GPU)",
    "Date": "Jun 13, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/GAMLP+SCR.pdf",
    "Paper Summary": "The paper presents the SCR (Semi-Supervised Consistency Regularization) framework aimed at enhancing the training of Graph Neural Networks (GNNs) through consistency regularization. The focus is on two primary strategies for implementing this framework:\n\n### 1. **Disagreement Minimization (SCR)**:\n   - This strategy entails reducing the discrepancies among predictions generated by varying versions of a GNN model. These versions can arise from the model's inherent randomness (like dropout) or through data augmentation techniques.\n   - It operates under the premise that perturbations in the input should not result in significantly differing predictions, reinforcing the concept of low-density separation, where decision boundaries ideally reside in regions of low data density.\n\n### 2. **Mean-Teacher Consistency Regularization (SCR-m)**:\n   - This method leverages a teacher-student model paradigm where a teacher network provides stability to the training process. The student model’s parameters are updated using an exponential moving average (EMA) of the teacher's parameters.\n   - The consistency loss in this approach is computed between the predictions of the student and the teacher, rather than minimizing direct prediction disagreements as in the SCR method.\n   - This assists in guiding the student model toward more reliable predictions, improving learning stability, especially in semi-supervised settings where labeled data is sparse.\n\n### **Noisy Prediction Generation**:\n   - SCR employs dropout as a simple method to generate noisy predictions of the same input. Multiple evaluations of the same input under different dropout conditions yield a set of noisy outputs, which aids in regularization and enhances generalization.\n\n### **Pseudo Labeling**:\n   - Pseudo labels are generated for unlabeled nodes based on the noisy predictions. In SCR, the pseudo label of a node is computed as the average of its noisy predictions, while in SCR-m, pseudo labels are derived from the teacher network’s predictions.\n   - After obtaining pseudo labels, a sharpening function is applied to reduce their entropy, encouraging the model to focus on high-confidence predictions.\n\n### **Loss Function Design**:\n   - The total loss is composed of two components: one for the labeled nodes (using standard cross-entropy loss) and another for unlabelled nodes (consistency loss based on the distance between predictions and pseudo labels). This structure ensures the model efficiently utilizes both labeled and unlabeled data during training.\n\n### **Confidence-Based Masking**:\n   - This mechanism selectively involves confident predictions from the pseudo labeled data in the training process, filtering out less reliable ones to stabilize training.\n\nIn summary, the SCR framework combines innovative strategies like disagreement minimization, a teacher-student approach, and confidence-based mechanisms to enhance the effectiveness of GNN training in scenarios where labeled data is limited. The design emphasizes generating and utilizing predictions under various conditions to optimize learning outcomes while ensuring generalization."
}