{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 15,
    "Method": "PCAPass + LightGBM",
    "External Data": "No",
    "Test Accuracy": "0.6591 ± 0.0003",
    "Validation Accuracy": "0.6982 ± 0.0002",
    "Contact": "mailto:krzysztof.sadowski@intel.com",
    "Paper Link": "https://arxiv.org/abs/2202.00408",
    "Code Link": "https://github.com/ksadowski13/PCAPass",
    "Parameters": "0",
    "Hardware": "Intel Xeon 6330 (CPU 1TB)",
    "Date": "Feb 9, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/PCAPass_+_LightGBM.pdf",
    "Paper Summary": "The paper introduces a novel method called **PCAPass** for generating graph node embeddings by integrating **Principal Component Analysis (PCA)** with **message passing** techniques. The model design aspects of PCAPass can be summarized as follows:\n\n1. **Combining PCA and Message Passing**: PCAPass employs PCA to reduce the dimensionality of node feature representations. This dimensionality reduction is applied iteratively to preserve significant variations in the data while preparing it for message passing. \n\n2. **Skip Connections**: The model incorporates skip connections, which facilitate the retention of previously obtained node embeddings. By concatenating the current node embedding with the previous one, PCAPass mitigates the issues of **over-smoothing** and **over-squashing** commonly found in conventional GNNs.\n\n3. **Message Aggregation**: The architecture utilizes an aggregation function that gathers information from the **k-hop neighborhood** of each node. The aggregate operation is performed on the node features obtained from local neighbors, allowing the model to consider both the current and preceding embeddings. \n\n4. **Graph Structure Utilization**: PCAPass leverages the graph structure effectively during preprocessing, leading to a more computationally efficient process compared to traditional GNNs, which necessitate message passing during training. \n\n5. **Gradient Boosted Decision Trees (GBDT)**: After the embeddings are generated through PCAPass, they are processed utilizing GBDT (specifically, the XGBoost algorithm). This choice emphasizes the model’s design where the GBDT classifier exploits the structure of the graph in a non-neural network fashion, thus providing flexibility and efficiency in handling heterogeneous features.\n\n6. **Layered Structure**: The design consists of multiple iterations (k-hop layers) of the message aggregation and dimensionality reduction processes. In each iteration, PCA is applied to the concatenated embeddings to ensure that essential information from the neighborhood and the historical node states are preserved.\n\n7. **Avoiding Node Sampling**: Unlike many approaches that involve node sampling or graph partitioning, PCAPass does not introduce any such biases into the training procedure. Instead, it maintains a straight use of the entire graph topology in its operations.\n\n8. **Customizable Aggregation Functions**: While mean aggregation is utilized in the experiments, the architecture allows for the integration of other aggregation functions that may be better suited to specific tasks or graph structures, enhancing its adaptability.\n\nIn essence, PCAPass is designed to optimize how node embeddings are generated by combining the strengths of PCA and message passing while addressing common pitfalls such as over-smoothing and over-squashing, thereby retaining crucial information across extended neighborhood distances in a graph."
}