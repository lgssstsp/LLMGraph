{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-proteins",
    "Dataset Link": "../nodeprop/#ogbn-proteins",
    "Rank": 16,
    "Method": "DeepGCN",
    "External Data": "No",
    "Test Accuracy": "0.8496 ± 0.0028",
    "Validation Accuracy": "0.8971 ± 0.0011",
    "Contact": "mailto:guohao.li@kaust.edu.sa",
    "Paper Link": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNNs_ICCV_2019_paper.pdf",
    "Code Link": "https://github.com/lightaime/deep_gcns_torch/blob/master/examples/ogb/ogbn_proteins",
    "Parameters": "2,374,456",
    "Hardware": "NVIDIA Tesla V100 (32GB GPU)",
    "Date": "Jun 20, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-proteins/DeepGCN.pdf",
    "Paper Summary": "The paper \"DeepGCNs: Can GCNs Go as Deep as CNNs?\" presents innovative methodologies designed to enable the successful training of very deep Graph Convolutional Networks (GCNs) by borrowing concepts from Convolutional Neural Networks (CNNs). The key model design aspects discussed in the article include:\n\n### 1. Residual Connections\n- The authors introduce residual connections to mitigate the vanishing gradient problem commonly encountered when stacking deep layers in GCNs. This technique enhances the flow of gradients during backpropagation, leading to more stable training and better convergence. The residual learning framework involves mapping an input graph to an output residual graph, allowing the network to learn more complex transformations without losing essential information.\n\n### 2. Dense Connections\n- Inspired by DenseNet architectures, dense connections are employed to facilitate improved information flow between layers. Unlike traditional GCNs where each layer's output is independent, dense connections concatenate the features from all preceding layers, thus enriching the feature set available for subsequent layers. This approach not only enhances the expressiveness of the model but also makes better use of features learned in earlier layers.\n\n### 3. Dilated Convolutions\n- Dilated convolutions are introduced to increase the receptive field of the GCN without losing resolution. The technique employs a dilated k-nearest neighbor (k-NN) mechanism, where edges are dynamically updated at each layer, allowing the model to capture multi-scale contextual information effectively. This is particularly useful in tasks such as semantic segmentation where a broader contextual understanding is critical.\n\n### 4. Dynamic k-NN Graph Construction\n- Traditional GCNs often rely on fixed graph structures. In contrast, the proposed methods utilize dynamic graph convolution that recalculates the nearest neighbors after every layer, allowing the network to adaptively learn the structure of the graph. This dynamic adjustment helps alleviate over-smoothing, a common issue in deeper GCNs, and enables a more efficient learning process as it allows the model to learn better representations from varying neighborhood configurations.\n\n### 5. Combination of Methods\n- The paper emphasizes integrating all three methods—residual connections, dense connections, and dilated convolutions—in GCN architectures, leading to a more robust framework capable of training models with 56 layers. This combination effectively addresses limitations faced by existing GCN models and provides a pathway for handling complex non-Euclidean data structures.\n\n### Model Architectures\n- The architecture includes distinct blocks: a GCN backbone block for feature extraction, a fusion block for aggregating multi-scale features, and a multi-layer perceptron (MLP) prediction block for final predictions. Different variations of GCNs (PlainGCN, ResGCN, DenseGCN) are structured based on the presence or absence of these advanced connection techniques, enabling an extensive analysis of their impact on model performance.\n\nIn summary, the article illustrates how advancements from CNNs can be effectively integrated into GCNs to facilitate deeper graph networks while reducing training complexities. These design innovations promise to enhance the applicability of GCNs in various challenging domains."
}