{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-proteins",
    "Dataset Link": "../nodeprop/#ogbn-proteins",
    "Rank": 19,
    "Method": "GraphSAGE",
    "External Data": "No",
    "Test Accuracy": "0.8221 ± 0.0032",
    "Validation Accuracy": "0.8831 ± 0.0044",
    "Contact": "mailto:luoyk@buaa.edu.cn",
    "Paper Link": "https://arxiv.org/pdf/2406.08993",
    "Code Link": "https://github.com/LUOyk1999/tunedGNN",
    "Parameters": "2,444,896",
    "Hardware": "GeForce RTX 3090 (24GB GPU)",
    "Date": "Jun 15, 2024",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-proteins/GraphSAGE.pdf",
    "Paper Summary": "The paper \"Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification\" by Yuankai Luo et al. focuses on the methodologies related to classic Graph Neural Networks (GNNs), specifically GCN, GAT, and GraphSAGE, highlighting critical design aspects that contribute to their performance in node classification tasks.\n\n1. **Model Architecture**:\n   - **Graph Convolutional Networks (GCN)**: Utilizes a layer that performs a form of convolution on graph data by aggregating node features from neighboring nodes and applying a weight matrix followed by a non-linear activation function. The model can be expressed mathematically, capturing the essence of message-passing in graphs.\n   - **Graph Attention Networks (GAT)**: Implements a self-attention mechanism that assigns varying weights to neighboring nodes during aggregation. This results in a focus on more informative neighbors and helps in capturing the significance of different edges in the graph structure.\n   - **GraphSAGE**: Introduces a sampling mechanism that condenses neighbor information to construct embeddings, allowing the model to scale efficiently on large graphs. It computes representations by combining the node’s own features and the mean features from its neighbors, optimized through trainable matrices.\n\n2. **Hyperparameters**:\n   - The study emphasizes several key hyperparameters that significantly influence the performance of GNNs:\n     - **Normalization**: Techniques such as Batch Normalization or Layer Normalization are crucial for mitigating issues like covariate shift during the training process, especially in large-scale graphs, enabling faster convergence and more stable outputs.\n     - **Dropout**: Applied to combat overfitting, dropout rates help in maintaining robustness by preventing co-adaptation of features across the network.\n     - **Residual Connections**: Integrating residual connections helps to combat the vanishing gradient problem and over-smoothing by allowing gradients to flow more freely, which is particularly useful on heterophilous graphs.\n     - **Network Depth**: The depth of the GNN is critically analyzed, suggesting that while deeper architectures may offer advanced feature processing, they also present unique challenges, such as increased over-smoothing. The study proposes an effective range of depths for achieving optimal performance in various graph types.\n     - **Jumping Knowledge**: This mode allows the model to aggregate features from different layers, although its influence is observed to be minimal in large-scale models, indicating that the architecture may not benefit as much from deep representations as previously thought.\n\n3. **Model Configuration**:\n   - The authors provide a comprehensive outline of configurations across their GNN variants, detailing choices regarding hyperparameters specific to each dataset and architecture. This includes parameters for dropout rates, the use of normalization techniques, and selection of the number of layers, facilitating a structured approach for tuning GNNs effectively.\n\nThrough these methodological insights, the authors underscore the potential of classic GNNs, asserting that with proper hyperparameter tuning and architectural decisions, they can achieve competitive results against more advanced models like Graph Transformers (GTs). The paper aims to promote rigorous and methodical evaluations within the graph machine learning community, providing essential knowledge about model design choices that significantly impact performance."
}