{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-biokg",
    "Dataset Link": "../linkprop/#ogbl-biokg",
    "Rank": 10,
    "Method": "DistMult",
    "External Data": "No",
    "Test Accuracy": "0.8043 ± 0.0003",
    "Validation Accuracy": "0.8055 ± 0.0003",
    "Contact": "mailto:hyren@cs.stanford.edu",
    "Paper Link": "https://arxiv.org/abs/1412.6575",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/biokg",
    "Parameters": "187,648,000",
    "Hardware": "GeForce RTX 2080 (11GB GPU)",
    "Date": "Jun 10, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-biokg/DistMult.pdf",
    "Paper Summary": "The paper presents a unified framework for learning representations of entities and relations in knowledge bases (KBs) using neural-embedding methods. Key contributions and methodologies for model design include:\n\n1. **General Framework for Multi-Relational Learning**: The authors propose a neural network framework that unifies several multi-relational embedding models, specifically NTN and TransE. This framework allows for various designs in entity and relation representations.\n\n2. **Entity Representations**: Entities can be represented using high-dimensional vectors, either from one-hot or n-hot index encoding. The first layer of the neural network projects these input vectors to low-dimensional representations. These representations can be learned through both linear and non-linear functions.\n\n3. **Relation Representations**: The choice of relation representation is encapsulated in the scoring function. The paper details several scoring functions that can be represented as linear transformations (for instance, \\(g_a\\)) or bilinear transformations (like \\(g_b\\)). The authors explore the simple bilinear form as a foundational model, stating that it uses a scoring function defined as:\n   \\[\n   g_b(y_{e1}, y_{e2}) = y_{e1}^T M y_{e2}\n   \\]\n   where \\(M\\) relates to the representation of the relation. They also discuss a diagonal matrix approach to reduce the complexity of parameters while maintaining performance.\n\n4. **Parameter Learning**: The parameters of the various models are learned by minimizing a margin-based ranking objective. This encourages positive relationships (triplets) to have higher scores than negative ones, which are generated by corrupting either the subject or object within the triplet.\n\n5. **Embedding-Based Rule Extraction**: The learned embeddings are also utilized for extracting logical rules from the KB. The authors outline how to model relation composition via multiplication or addition of relation embeddings, leveraging embeddings as vectors (for translational models like TransE) or matrices (for bilinear models).\n\n6. **Search Space Reduction for Rule Extraction**: The embedding-based rule extraction method efficiently explores the search space by considering domain information (the types of entities allowed for each relation), which helps avoid computational overhead while maximizing the relevance of discovered rules.\n\nUltimately, the paper emphasizes a systematic approach to entity and relation representation, advocating for the use of bilinear scoring functions and embeddings that facilitate both effective representation learning and logical reasoning capabilities in multi-relational datasets."
}