{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-ddi",
    "Dataset Link": "../linkprop/#ogbl-ddi",
    "Rank": 4,
    "Method": "AGDN (AUC loss)",
    "External Data": "No",
    "Test Accuracy": "0.9538 ± 0.0094",
    "Validation Accuracy": "0.8943 ± 0.0281",
    "Contact": "mailto:chuxiongsun@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2012.15024",
    "Code Link": "https://github.com/skepsun/Adaptive-Graph-Diffusion-Networks",
    "Parameters": "3,506,691",
    "Hardware": "Tesla V100 (16GB GPU)",
    "Date": "Sep 2, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-ddi/AGDN_(AUC_loss).pdf",
    "Paper Summary": "The paper presents **Adaptive Graph Diffusion Networks (AGDNs)**, a novel architectural design aimed at improving the performance and efficiency of Graph Neural Networks (GNNs). Key aspects of the model design are summarized as follows:\n\n### 1. **Graph Diffusion Framework**\n- **Graph Diffusion Networks (GDNs)**: AGDNs build upon the foundational concept of GDNs, which replace standard graph convolution operations with a more flexible graph diffusion approach. This allows for leveraging multi-hop information while maintaining a manageable complexity.\n\n### 2. **Adaptive Mechanisms**\n- **Weighting Coefficients**: The AGDN architecture introduces learnable and scalable mechanisms for hop-wise weighting. Instead of fixed coefficients, AGDN employs two methods to adaptively learn the importance of different hops:\n  \n  - **Hop-wise Attention (HA)**: This mechanism uses a learnable query vector to compute attention scores, yielding a weighting matrix normalized along the hops. This allows different nodes to receive weights that reflect their specific importance in the context of the graph.\n  \n  - **Hop-wise Convolution (HC)**: Instead of using a uniform weighting scheme across all hops, HC allows different feature channels to have adaptive weights. This results in a convolution-like layer that learns different coefficients for each hop and channel, enhancing the model's expressiveness.\n\n### 3. **Layer Design**\n- **Multi-layer Graph Diffusion**: AGDNs are constructed by stacking multiple AGDN layers, where each layer incorporates multi-hop representations. The design allows the captured information to propagate through different layers while reducing potential overfitting or over-smoothing problems commonly associated with deep GNNs.\n\n### 4. **Transition Matrix**\n- AGDNs modify the transition matrix, introducing a pseudo-symmetric normalized GAT adjacency to better integrate information from source and destination nodes. This matrix captures both in-degrees and out-degrees, facilitating improved aggregation of node features across hops.\n\n### 5. **Complexity Management**\n- The model aims to strike a balance between expressive power and computational efficiency. By focusing on shallow transformations with the graph diffusion approach and limiting the number of feature transformations, AGDN manages to maintain lower memory usage and runtime compared to deeper models like Reversible GNNs.\n\n### 6. **Flexibility and Generalization**\n- The architectural design of AGDNs promotes flexibility in how weights and features are learned across nodes and layers. This adaptability is essential for handling different types of graph structures and learning tasks without the burden of excessive model complexity.\n\nIn summary, AGDNs enhance traditional GNNs by incorporating adaptive graph diffusion mechanisms that allow for multi-hop feature learning with varying importance, optimizing layer construction, and implementing a robust transition matrix, all while managing computational complexity."
}