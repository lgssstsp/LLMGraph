{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 1,
    "Method": "GLEM+GIANT+GAMLP",
    "External Data": "Yes",
    "Test Accuracy": "0.7037 ± 0.0002",
    "Validation Accuracy": "0.7354 ± 0.0001",
    "Contact": "mailto:andy.zhaoja@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2210.14709",
    "Code Link": "https://github.com/AndyJZhao/GLEM",
    "Parameters": "154,775,375",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Nov 9, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/GLEM+GIANT+GAMLP.pdf",
    "Paper Summary": "The paper introduces a novel framework called GLEM (Graph and Language Learning by Expectation-Maximization) aimed at learning effective node representations on text-attributed graphs (TAGs) by integrating large language models (LMs) and graph neural networks (GNNs) through a variational Expectation-Maximization (EM) approach.\n\n### Model Design Aspects:\n\n1. **Framework Structure**:\n   - GLEM alternates between two primary modules: the language model (LM) and the graph neural network (GNN).\n   - The process consists of two phases:\n     - **E-step**: The LM is optimized while keeping the GNN parameters fixed. In this step, the LM predicts labels for nodes based on their textual information and the pseudo-labels inferred from the GNN.\n     - **M-step**: The GNN is optimized by leveraging the node representations provided by the LM and the pseudo-labels generated by it. This updates the GNN to utilize both textual and structural information for label prediction.\n\n2. **Variational EM Framework**:\n   - GLEM employs a pseudo-likelihood variational framework, which aims to maximize the log-likelihood function of observed node labels. \n   - The optimization procedure involves iteratively optimizing the two modules, E-step for LM and M-step for GNN, by fixing one while updating the other.\n\n3. **Label Distribution Modeling**:\n   - The LM models label distributions conditioned on the local text attributes of each node, while the GNN defines a global conditional label distribution using text and label information from surrounding nodes.\n   - During E-step, the LM is trained to predict both true labels and GNN-predicted pseudo-labels.\n   - During M-step, the GNN uses the textual embeddings and the predicted labels from the LM to refine its predictions.\n\n4. **Pseudo-labeling Mechanism**:\n   - Pseudo-labels generated by the LM and GNN allow for mutual enhancement, where the LM learns from the global structure conveyed by the GNN, and the GNN incorporates local semantic information from the LM for label prediction.\n\n5. **Parameterization of Distributions**:\n   - For the variational distribution \\( q(y | s) \\), the LM utilizes a transformer-based architecture to define the label distributions of unlabeled nodes.\n   - The GNN defines its labeling distribution \\( p(y | s, A, y) \\) by incorporating both the adjacency structure and textual information of nodes, enabling effective aggregation through the message-passing mechanism.\n\n6. **Training Objective Functions**:\n   - The objectives for both the LM and GNN involve two components: one for utilizing labeled nodes and the other for guiding the models through pseudo-labels.\n   - Hyperparameters \\(\\alpha\\) and \\(\\beta\\) balance the contributions of true labels and pseudo-labels during training.\n\n7. **Scalability Considerations**:\n   - GLEM’s structure enables it to handle large-scale TAGs effectively, avoiding the high memory costs associated with simultaneous training of large LMs and GNNs. This is achieved through the alternate updates of the two models which optimize resources without sacrificing interaction between the models.\n\nOverall, GLEM emphasizes a robust integration of LMs and GNNs to create a scalable and effective model for node representation learning on text-attributed graphs, leveraging both text and structural information coherently in its design."
}