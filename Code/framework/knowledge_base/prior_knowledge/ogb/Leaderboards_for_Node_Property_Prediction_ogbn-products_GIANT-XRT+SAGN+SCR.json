{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 7,
    "Method": "GIANT-XRT+SAGN+SCR",
    "External Data": "Yes",
    "Test Accuracy": "0.8667 ± 0.0009",
    "Validation Accuracy": "0.9364 ± 0.0005",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/SCR",
    "Parameters": "1,154,654",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Jun 13, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GIANT-XRT+SAGN+SCR.pdf",
    "Paper Summary": "The paper presents a framework called SCR (Consistency Regularization) designed to enhance the training of Graph Neural Networks (GNNs) in semi-supervised settings by employing consistency regularization techniques. SCR specifically focuses on balancing the error from labeled data and unlabeled data during training.\n\n### Key Components of SCR Framework:\n\n1. **Consistency Regularization Strategies**:\n   - **SCR**: This method involves minimizing the disagreement among predictions from different augmented versions of the GNN model. It achieves this by generating multiple noisy predictions through techniques such as dropout, then calculating a consistency loss to ensure that predictions remain close to each other across these variations.\n   - **Mean-Teacher Consistency Regularization (SCR-m)**: A variation where predictions from a 'teacher' model, which is an exponentially moving average of the 'student' model parameters, are used to guide the training process. This method also calculates a consistency loss to align predictions made by the student model with those of the teacher.\n\n2. **Prediction Generation and Pseudolabeling**:\n   - SCR generates noisy predictions for a given input graph through dropouts, producing multiple outputs for the same node. \n   - For pseudolabeling, SCR uses the average of these noisy predictions to define the pseudolabel, while SCR-m utilizes the predictions from the teacher model for this purpose.\n\n3. **Loss Function Design**:\n   - The total loss function in SCR comprises two parts: \n     - A cross-entropy loss calculated using labeled nodes.\n     - A consistency regularization term that utilizes the distance between the pseudolabels and the predictions derived from unlabeled nodes. This ensures that pseudolabels, which indicate where the model is less confident, are treated carefully to avoid degrading model performance.\n\n4. **Confidence-based Masking**:\n   - To mitigate the influence of unreliable pseudolabels, a confidence-based masking strategy is introduced. This involves filtering out nodes that yield low confidence predictions based on a defined threshold, ensuring that only confident predictions contribute to the consistency loss.\n\n5. **Sharpening Function**:\n   - A sharpening function is employed on the generated pseudolabels to reduce entropy, encouraging the model to produce lower-entropy predictions (i.e., higher confidence decisions).\n\n6. **Training Procedure**:\n   - The training consists of selecting a mix of labeled and unlabeled nodes in each iteration, computing noisy predictions via dropouts, generating pseudolabels, and finally calculating the total loss to guide the model updates.\n\nThe SCR framework is general such that it can be applied to a variety of GNN architectures and is designed to scale efficiently, making it suitable for handling large graph datasets. It notably includes mechanisms allowing for adaptive thresholding and gradual updates to improve the model's stability during training. Overall, SCR and its variant SCR-m are designed to be flexible and effective in improving the performance of GNNs under semi-supervised learning conditions."
}