{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-vessel",
    "Dataset Link": "../linkprop/#ogbl-vessel",
    "Rank": 18,
    "Method": "MLP",
    "External Data": "No",
    "Test Accuracy": "0.4794 ± 0.0133",
    "Validation Accuracy": "0.4801 ± 0.0132",
    "Contact": "mailto:julian.mcginnis@tum.de",
    "Paper Link": "https://arxiv.org/abs/2005.00687",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/vessel",
    "Parameters": "1,037,577",
    "Hardware": "Quadro RTX 8000 Ti (48GB GPU)",
    "Date": "Aug 19, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-vessel/MLP.pdf",
    "Paper Summary": "### Summary of Model Design Aspects from the Open Graph Benchmark Paper\n\nThe paper focuses on the design of models and methodologies for graph machine learning (ML) through the Open Graph Benchmark (OGB). Below are key aspects of model design discussed within the article:\n\n#### 1. **Unified Dataset Construction**\n   - OGB datasets are constructed to ensure they are representative of various applications, including social networks, biological data, molecular graphs, and knowledge graphs.\n   - Each dataset supports multiple prediction tasks, namely:\n     - Node Property Prediction\n     - Link Property Prediction\n     - Graph Property Prediction\n\n#### 2. **Task-Specific Data Loading and Evaluation Protocols**\n   - The OGB package provides standardized data loaders designed for easy loading and compatibility with PyTorch and associated graph libraries (e.g., PyTorch Geometric, Deep Graph Library). This facilitates seamless integration into the ML models.\n   - Each dataset is associated with a specific evaluation metric relevant to the prediction task, ensuring consistency in performance indicators.\n\n#### 3. **Baseline Models and Architectures**\n   - The article presents various baseline models for different tasks:\n     - **Multilayer Perceptron (MLP)**: A basic model that uses input features directly without leveraging any graph structure.\n     - **Graph Neural Networks (GNNs)**: Several architectures are discussed, including:\n       - **Graph Convolutional Network (GCN)**: Utilizes convolutional layers adapted for graph structures.\n       - **GraphSAGE**: Introduces neighborhood sampling techniques during training, allowing for scalable learning on large graphs.\n       - **Graph Isomorphism Network (GIN)**: A model designed to be more expressive in capturing graph structures.\n\n#### 4. **Training Techniques and Innovations**\n   - **Mini-batch Training**: Various mini-batch training methods like Neighbor Sampling, Cluster-GCN, and GraphSAINT are introduced, allowing GNNs to handle large datasets efficiently.\n   - **Use of Virtual Nodes**: Some models incorporate virtual nodes to enhance message-passing through augmented graphs, enabling the models to retain central node information.\n\n#### 5. **Advanced Representation Learning**\n   - GNNs are used to capture node embeddings, with methods for pooling to derive embeddings for entire graphs.\n   - For tasks where edge features are significant, models can integrate edge-based information during the training phase, improving the capability of models to discern relationships.\n\n#### 6. **Handling Heterogeneous Graphs**\n   - Models like R-GCN are specifically designed to handle heterogeneous graphs, utilizing different parameters for different edge types and employing meta-path-based approaches for node embeddings.\n\n#### 7. **Feature Engineering**\n   - The design highlights the importance of input features, which include not just raw node features but also engineered features like additional node attributes and edge characteristics. This is particularly relevant for molecular datasets where chemical properties may greatly influence model performance.\n\n#### 8. **Diversity in Graph Representations**\n   - The paper emphasizes the need for models to adapt to the diverse structural characteristics of graphs across different datasets, including variabilities in density, clustering, and node connections.\n\nOverall, the article sets a framework that not only addresses model design aspects but also lays out a comprehensive approach to facilitate reproducible and scalable graph ML research via the OGB initiative. This includes customized dataset creation, a unified evaluation framework, and a variety of model architectures aimed at improving generalization and performance on graph-based tasks."
}