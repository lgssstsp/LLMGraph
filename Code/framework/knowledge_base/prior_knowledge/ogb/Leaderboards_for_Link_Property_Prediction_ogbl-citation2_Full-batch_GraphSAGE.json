{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-citation2",
    "Dataset Link": "../linkprop/#ogbl-citation2",
    "Rank": 16,
    "Method": "Full-batch GraphSAGE",
    "External Data": "No",
    "Test Accuracy": "0.8260 ± 0.0036",
    "Validation Accuracy": "0.8263 ± 0.0033",
    "Contact": "mailto:matthias.fey@tu-dortmund.de",
    "Paper Link": "https://arxiv.org/abs/1706.02216",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/citation2",
    "Parameters": "460,289",
    "Hardware": "Quadro RTX 8000 (48GB GPU)",
    "Date": "Jan 4, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-citation2/Full-batch_GraphSAGE.pdf",
    "Paper Summary": "The paper introduces **GraphSAGE** (Sample and Aggregate), a novel framework for inductive representation learning on large graphs. This method aims to efficiently generate embeddings for unseen nodes by leveraging node feature information, allowing the approach to generalize across varying graph structures.\n\n### Model Design Aspects\n\n1. **Inductive Learning Framework**:\n   - Unlike traditional transductive methods, GraphSAGE generates embeddings for unseen nodes by learning a function that aggregates feature information from a node's local neighborhood instead of precomputing embeddings for each node.\n\n2. **Aggregation Functions**:\n   - The key innovation lies in using aggregator functions that sample and aggregate features from a node's surrounding nodes. This allows flexibility and scalability when encountering new or evolving graphs. Three types of aggregators are explored:\n     - **Mean Aggregator**: Computes the element-wise mean of neighbor vectors. Similar to traditional convolutional approaches, though without the concatenation of the current node's representation.\n     - **LSTM Aggregator**: Utilizes an LSTM architecture to enhance expressive capacity. It processes unordered node neighbor features by applying LSTMs to a random permutation of the neighbors.\n     - **Pooling Aggregator**: Performs a max-pooling operation after transforming each neighbor's vector through a fully connected network. This allows the model to capture different aspects of the neighborhood set.\n\n3. **Neighborhood Sampling**:\n   - GraphSAGE employs fixed-size uniform sampling of neighbors rather than full neighborhood sets, which maintains computational efficiency. This sampling is performed at each iteration of aggregating neighbors, ensuring that the processing footprint is controlled.\n\n4. **Loss Function and Training**:\n   - The approach utilizes an unsupervised loss function to train the system, designed to encourage similar representations for close nodes while maintaining distinctions for distant nodes. This allows GraphSAGE to learn embeddings in an unsupervised manner, making it adaptable for specialized downstream tasks.\n   - Parameters for the aggregator functions are learned using standard stochastic gradient descent techniques, making the training process efficient and scalable.\n\n5. **Embedding Generation**:\n   - At inference time, after training, the learned aggregation functions can be applied to generate embeddings for entirely unseen nodes based on their local neighborhoods. The process involves several layers (defined by the depth \\(K\\)) where nodes iteratively aggregate information from their neighbors.\n\n6. **Model Architecture**:\n   - The model's capacity to learn about graph structures is bolstered by the symmetry of the aggregator functions, allowing the framework to be applied to arbitrarily ordered sets of node features. This symmetry is critical as it ensures the model's training is invariant to changes in the order of neighbor inputs.\n\nThe GraphSAGE framework showcases a flexible, efficient method of generating node embeddings that effectively handles new and varying graph structures, bridging the gap between traditional transductive methods and the needs of dynamic real-world applications."
}