{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-collab",
    "Dataset Link": "../linkprop/#ogbl-collab",
    "Rank": 13,
    "Method": "SEAL-nofeat (val as input)",
    "External Data": "No",
    "Test Accuracy": "0.6474 ± 0.0043",
    "Validation Accuracy": "0.6495 ± 0.0043",
    "Contact": "mailto:muhan.zhang@hotmail.com",
    "Paper Link": "https://arxiv.org/pdf/2010.16103.pdf",
    "Code Link": "https://github.com/facebookresearch/SEAL_OGB",
    "Parameters": "501,570",
    "Hardware": "NVIDIA Tesla V100 (32GB GPU)",
    "Date": "Jun 16, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-collab/SEAL-nofeat_(val_as_input).pdf",
    "Paper Summary": "The paper \"Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning\" proposes a theoretical framework for improving multi-node representation learning using Graph Neural Networks (GNNs). The main focus is on addressing the limitations of commonly used methods that directly aggregate single-node representations to form a multi-node representation, which fails to consider the dependence between nodes in a set.\n\n### Key Methods and Design Aspects:\n\n1. **Direct Aggregation of Node Representations**:\n   - Many existing methods, like Graph AutoEncoder (GAE), compute individual node representations independently and then aggregate them (for example, using inner products, sums, or means) to predict relationships between multiple nodes (like links). This approach is shown to have a fundamental limitation: it cannot capture the structural relationships and dependencies among the nodes in the representation.\n\n2. **Labeling Trick**:\n   - The labeling trick is proposed as a solution to the limitations of direct aggregation methods. It involves the labeling of nodes in a way that distinguishes nodes in the target set from others. For instance, by applying a labeling tensor to a graph, it enhances a GNN's capacity to learn more expressive representations.\n   - Two crucial properties are required for a valid labeling function:\n     - **Target-Nodes-Distinguishing**: The labels for the nodes in the target set should be distinct from those of other nodes.\n     - **Permutation Equivariance**: The labels should change consistently with permutations of the nodes, maintaining their relationship to the original graph structure.\n\n3. **Specific Implementations of Labeling Trick**:\n   - **Zero-One Labeling**: A straightforward implementation where nodes in the target set receive a label of 1, while all others receive a label of 0. This helps ensure that when computing node representations, GNNs \"know\" which nodes to focus on.\n   - **Distance-Based Labeling**: Methods like *Double Radius Node Labeling* (DRNL) assign labels based on the distance of nodes to the target set, thus ensuring structural information is utilized.\n\n4. **Structural Representations**:\n   - The paper emphasizes that a valid labeling trick enables a GNN to learn \"most expressive structural representations\" of node sets. The main theorem states that with an appropriately expressive GNN and injective aggregation functions, aggregating node representations from a labeled graph results in a structural representation for the node set.\n\n5. **Local Isomorphism**:\n   - The concept of “local h-isomorphism” is introduced, which generalizes standard isomorphism by only requiring that the h-hop enclosing subgraphs around nodes be isomorphic. This concept allows for greater flexibility in applying the proposed methods to more practical scenarios where exact isomorphism is rare.\n\nOverall, the proposed method design revolves around the notion that capturing the dependencies between nodes in a multi-node set—through careful node labeling—allows GNNs to produce more informative and discriminating representations compared to methods that rely solely on aggregating single-node representations."
}