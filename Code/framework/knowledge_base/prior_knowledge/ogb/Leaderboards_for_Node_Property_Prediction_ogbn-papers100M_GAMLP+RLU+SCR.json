{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-papers100M",
    "Dataset Link": "../nodeprop/#ogbn-papers100M",
    "Rank": 3,
    "Method": "GAMLP+RLU+SCR",
    "External Data": "No",
    "Test Accuracy": "0.6842 ± 0.0015",
    "Validation Accuracy": "0.7188 ± 0.0007",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/SCR",
    "Parameters": "67,560,875",
    "Hardware": "GeForce RTX 3090 24GB (GPU)",
    "Date": "Jun 13, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-papers100M/GAMLP+RLU+SCR.pdf",
    "Paper Summary": "The paper presents the **SCR (Consistency Regularization) framework** designed to improve the performance of graph neural networks (GNNs) through two primary strategies:\n\n### 1. SCR Strategy\n- **Objective:** Minimize disagreements among perturbed predictions generated by different versions of a GNN model under the same input.\n- **Execution:** \n  - Employs data augmentation or model variability (like dropout) to create multiple noisy predictions from the same input.\n  - These predictions are expected to be similar due to the low-density separation assumption—the belief that small changes in input do not drastically change model outputs.\n  - The strategy effectively improves the generalization ability of GNN models.\n\n### 2. Mean-Teacher Consistency Regularization (SCR-m)\n- **Objective:** Enhance consistency using a teacher-student model framework.\n- **Execution:**\n  - A teacher model, updated via an Exponential Moving Average (EMA) from the student model, generates pseudo-labels for the unlabeled nodes.\n  - During training, the consistency loss is computed between predictions from the student model and those predicted by the teacher, rather than simply minimizing disagreement among the predictions.\n  - The use of EMA allows for a more stable learning process, as updates to the teacher model lag behind the student, thus preventing instability caused by rapid changes in model weights.\n\n### Model Training Process\n- The SCR framework integrates these strategies into the training of GNNs by:\n  1. **Noisy Prediction Generation:** Implementing dropout to obtain a diverse set of predictions for each input.\n  2. **Pseudo-Labeling:** \n     - In SCR, the pseudo-label for an unlabeled node is the average of its noisy predictions.\n     - In SCR-m, the teacher model generates pseudo-labels based on its predictions.\n  3. **Confidence-Based Masking:** During training, a confidence mask is used to filter out unlabeled nodes whose predictions are unreliable, ensuring that the model focuses on high-confidence nodes.\n\n### Loss Function\nThe total loss function is comprised of:\n- A supervised loss that calculates the cross-entropy between the true labels of the labeled nodes and the predictions.\n- An unsupervised consistency loss that encourages the model’s predictions for unlabeled nodes to align with the generated pseudo-labels, focusing on confident predictions only.\n\n### Generalizability and Scalability\n- The SCR framework is designed to be applicable across various GNN architectures (e.g., SAGN, GAMLP, GraphSAGE, etc.), enhancing their performance and allowing for scalability to large graph datasets.\n\nThe SCR framework aims to enhance the training of GNNs in a semi-supervised manner, demonstrating an effective balance between leveraging labeled and unlabeled data through consistency regularization techniques."
}