{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-citation2",
    "Dataset Link": "../linkprop/#ogbl-citation2",
    "Rank": 17,
    "Method": "NeighborSampling (SAGE aggr)",
    "External Data": "No",
    "Test Accuracy": "0.8044 ± 0.0010",
    "Validation Accuracy": "0.8054 ± 0.0009",
    "Contact": "mailto:matthias.fey@tu-dortmund.de",
    "Paper Link": "https://arxiv.org/abs/1706.02216",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/citation2",
    "Parameters": "460,289",
    "Hardware": "GeForce RTX 2080 (11GB GPU)",
    "Date": "Jan 4, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-citation2/NeighborSampling_(SAGE_aggr).pdf",
    "Paper Summary": "The paper presents **GraphSAGE** (SAmple and agGreGatE), an inductive framework designed for generating node embeddings in large graphs. Unlike traditional transductive methods that require all nodes to be present during training, GraphSAGE allows for the efficient generation of embeddings for unseen nodes by leveraging features information. Here are the core components of its methodology:\n\n### Model Design Aspects:\n\n1. **Inductive Learning Approach:**\n   - GraphSAGE utilizes a function to generate embeddings based on local neighborhood information rather than training embeddings for each individual node. This allows the model to generalize to unseen nodes and evolving graphs effectively.\n\n2. **Neighborhood Aggregation:**\n   - The model aggregates feature information from a node's local neighborhood using various aggregator functions, enabling it to learn representations that incorporate both topological structure and node features.\n\n3. **Aggregator Functions:**\n   - GraphSAGE proposes several architectures to perform aggregation:\n     - **Mean Aggregator:** Computes the element-wise mean of the features from neighboring nodes. It is akin to the convolutional propagation rule used in Graph Convolutional Networks (GCNs).\n     - **LSTM Aggregator:** Employs a Long Short-Term Memory (LSTM) architecture to aggregate information, making it more expressive than simpler methods.\n     - **Pooling Aggregator:** Uses max-pooling, where each neighbor's vector is passed through a neural network and then aggregated through a max operation. This approach allows capturing diverse aspects of the neighborhood set.\n\n4. **Embedding Generation Algorithm:**\n   - The algorithm generates embeddings through iterative forward propagation. Initially, each node starts with its feature vector, and at each iteration, a node aggregates representations from its neighbors, and the aggregated vector is concatenated with its current representation before being transformed by a neural network layer. This process continues for a defined number of hops (search depth).\n\n5. **Flexibility and Generalization:**\n   - GraphSAGE can be adapted to operate in both supervised and unsupervised settings. It can use a graph-based loss function to ensure that similar nodes have similar embeddings while maintaining distinctiveness for dissimilar nodes.\n   - The model is designed to adapt to various types of graphs, including those lacking specific node features, by still leveraging topological attributes like node degrees.\n\n6. **Batch Training with Minibatches:**\n   - The framework allows for minibatch training, making it suitable for large-scale graphs by sampling neighborhoods efficiently. A function samples neighbors without redundancy, which reduces computational overhead and memory requirements.\n\nOverall, GraphSAGE’s design leverages local neighborhood structures and node features to create robust embeddings suitable for a variety of graph-based learning tasks, enabling efficient generalization to previously unseen nodes and graphs."
}