{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-molpcba",
    "Dataset Link": "../graphprop/#ogbg-mol",
    "Rank": 6,
    "Method": "Nested GIN+virtual node (ensemble)",
    "External Data": "No",
    "Test Accuracy": "0.3007 ± 0.0037",
    "Validation Accuracy": "0.3059 ± 0.0056",
    "Contact": "mailto:muhan.zhang@hotmail.com",
    "Paper Link": "https://arxiv.org/pdf/2110.13197.pdf",
    "Code Link": "https://github.com/muhanzhang/NestedGNN",
    "Parameters": "44,187,480",
    "Hardware": "NVIDIA Tesla V100 (32GB GPU)",
    "Date": "Nov 21, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-molpcba/Nested_GIN+virtual_node_(ensemble).pdf",
    "Paper Summary": "The paper introduces Nested Graph Neural Networks (NGNNs) to address the limitations of traditional Graph Neural Networks (GNNs) and the Weisfeiler-Lehman (1-WL) algorithm in representing and classifying graphs. Here’s a focused summary on the methods and model design aspects discussed:\n\n### NGNN Framework\n\n1. **Core Idea**: Instead of encoding a rooted subtree around each node (as done by standard GNNs), NGNN learns a representation based on rooted subgraphs around each node. This change enhances the expressive power of the GNN by better capturing local substructure information.\n\n2. **Model Structure**:\n   - **Inner (Base) GNNs**: The framework employs a base GNN for learning representations from the rooted subgraphs around each node. This requires the extraction of a height-h rooted subgraph around each target node, which allows for richer local neighborhood representation compared to rooted subtrees.\n   - **Pooling Layers**: Each base GNN operates on its extracted rooted subgraph independently. After processing, a subgraph pooling layer aggregates the representations from nodes within the subgraph to generate a final representation for the root node.\n   - **Outer GNN**: The node representations obtained from the base GNNs are then further processed by an outer GNN, typically a pooling layer, which aggregates them into a representation of the entire graph.\n\n### Key Design Choices\n\n1. **Base GNN Flexibility**: The choice of the base GNN is flexible, allowing the use of various GNN architectures, thus enabling users to enhance the base model's representation capacity with NGNN.\n\n2. **Subgraph Height (h)**: The height of the rooted subgraph (h) is a critical design parameter. It determines the local receptive field size for each node's representation. A balance is sought, as a small subgraph height may not capture enough context, while too large a height could include excessive graph information.\n\n3. **Message Passing Layers (l)**: The number of message-passing layers in the base GNN (l) can be adjusted independently of the subgraph height. The design encourages using more layers than height to enhance the node's representation by absorbing comprehensive information from the local subgraph.\n\n4. **Pooling Functions**: Different pooling functions can be employed to summarize node representations into final subgraph or graph representations. The paper mentions mean pooling as the primary choice, but also discusses center pooling, which uses the center node's representation if it sufficiently captures the subgraph's information.\n\n5. **Initial Feature Augmentation**: NGNN allows for augmenting initial node features derived from distance information between nodes and the root. This structural feature enhancement is leveraged to solidify the expressiveness of the node representations.\n\n### Theoretical Expression Power\n\nNGNN is demonstrated to be theoretically more powerful than 1-WL and conventional message passing GNNs because it can distinguish almost all r-regular graphs, where traditional GNNs fail. This is attributed to the richer information captured from the subgraph pooling which facilitates a more expressive representation learning.\n\n### Summary\n\nNGNN expands on traditional GNN architectures through a dual-level structure of base and outer GNNs, incorporating rooted subgraphs for node representation, allowing flexibility in design choices, and aiming for improved expressiveness in graph-based learning tasks. These design aspects, combined with the ability to augment information through structural features, position NGNN as a robust framework for enhancing graph representation learning."
}