{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 20,
    "Method": "SAGN+MCR",
    "External Data": "No",
    "Test Accuracy": "0.8441 ± 0.0005",
    "Validation Accuracy": "0.9325 ± 0.0004",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "2,179,678",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/SAGN+MCR.pdf",
    "Paper Summary": "The paper presents the **SCR framework** (Semi-supervised Consistency Regularization) for enhancing the training of Graph Neural Networks (GNNs) by employing consistency regularization, specifically in semi-supervised settings. The key components of the model design in SCR are as follows:\n\n### 1. **Consistency Regularization Strategies**\n   - **SCR (Standard Consistency Regularization):** This approach minimizes disagreements among perturbed predictions generated by different versions of a GNN model. The perturbations can come from data augmentations or randomness introduced during model training, such as using dropout. By averaging predictions over multiple noisy evaluations, SCR improves the generalization capability of the model.\n   \n   - **SCR-m (Mean-Teacher Consistency Regularization):** This variant uses a teacher-student paradigm. The teacher model's parameters are updated using exponential moving averages (EMA) of the student model parameters without backpropagation. The consistency loss is computed between predictions from the teacher and the perturbed outputs from the student, allowing for improved stability and performance.\n\n### 2. **Noisy Prediction Generation**\n   SCR employs dropout as a means to create multiple noisy predictions for each node. Each node's predictions are obtained through several evaluations under different dropout conditions. This approach provides a lightweight way to introduce noise without expensive data augmentation processes and is scalable for large graphs.\n\n### 3. **Pseudo-Labeling**\n   The framework generates pseudo-labels for unlabeled nodes based on the predictions obtained. For SCR, the pseudo-label for each unlabeled node is computed as the average of its multiple noisy predictions. In contrast, SCR-m uses the teacher model to derive the pseudo-labels. The model also employs a sharpening function to reduce the entropy of these pseudo-labels, encouraging more confident decisions.\n\n### 4. **Loss Function Structure**\n   The total loss in the SCR framework consists of two parts:\n   - **Supervised Loss:** Standard cross-entropy loss evaluated on labeled nodes.\n   - **Unsupervised Consistency Loss:** This loss is calculated based on the confident predictions of unlabeled nodes, ensuring that predictions are tightly aligned with the generated pseudo-labels.\n\n### 5. **Confidence-Based Masking**\n   To enhance performance when applying pseudolabels, a confidence-based masking strategy is used. The model selectively tunes the pseudo-labeling by filtering out low-confidence predictions, ensuring that only predictions meeting a specified confidence threshold contribute to the consistency loss.\n\n### 6. **Training Efficiency**\n   The SCR framework is designed to be computationally efficient, avoiding the excessive retraining that characterizes multi-stage methods like self-training. This efficiency allows it to scale effectively to large graphs.\n\n### 7. **Hyperparameter Considerations**\n   The framework includes hyperparameters such as:\n   - **λ:** Controls the weight of the consistency regularization term.\n   - **S:** The number of views (or noisy predictions) to generate per node.\n   - **η:** Threshold for confidence-based masking.\n   - **T:** Temperature parameter for sharpening the pseudo-labels.\n   Additional tailored parameters for SCR-m, including EMA decay rates, are also specified.\n\n### Conclusion\nThe SCR framework's design aims to effectively leverage unlabeled data through consistency regularization strategies while maintaining training efficiency and scalability, which facilitate better performance in semi-supervised learning settings for graph neural networks."
}