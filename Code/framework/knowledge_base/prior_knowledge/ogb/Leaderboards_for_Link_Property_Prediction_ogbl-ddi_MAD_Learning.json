{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-ddi",
    "Dataset Link": "../linkprop/#ogbl-ddi",
    "Rank": 17,
    "Method": "MAD Learning",
    "External Data": "No",
    "Test Accuracy": "0.6781 ± 0.0294",
    "Validation Accuracy": "0.7010 ± 0.0082",
    "Contact": "mailto:cf020031308@163.com",
    "Paper Link": "https://arxiv.org/abs/2102.05246",
    "Code Link": "https://github.com/cf020031308/mad-learning/blob/master/ogbl-ddi.py",
    "Parameters": "1,228,897",
    "Hardware": "Geforce GTX 1080 Ti (11GB GPU)",
    "Date": "Feb 10, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Link_Property_Prediction/ogbl-ddi/MAD_Learning.pdf",
    "Paper Summary": "The paper introduces a novel learning paradigm called Memory-Associated Differential (MAD) Learning, which aims to leverage the relationships and associations in training data to improve predictions in supervised learning tasks. The key methods and model design aspects discussed in the article are as follows:\n\n1. **Memory Component**: \n   - The MAD Learning framework integrates a memory component that stores all training data. This allows the model to reference past information during inference rather than relying solely on the learned mapping from features to labels.\n   - The stored data can be treated as \"known facts,\" which inform predictions about unknown labels based on similarities and differences.\n\n2. **Differential Learning**:\n   - The learning mechanism incorporates a differential equation to estimate changes in output labels based on input features. Specifically, it uses the first-order Taylor series approximation to predict the unknown labels by relating them to known ones while considering the differences.\n   - This process translates the learning of output labels into learning a differential function, which facilitates clearer geometric interpretations of the outputs.\n\n3. **Inference Mechanism**:\n   - During the inference phase, MAD Learning utilizes the memory component to retrieve relevant known labels (memories) and make predictions based on learned differences and associations. This approach involves applying a visually interpretable geometric method for label prediction.\n\n4. **Sampling Techniques**:\n   - MAD Learning explores multiple sampling methods to enhance the effectiveness of reference selection for the predictions. These include:\n     - **Multi-head Sampling**: The use of multiple instances to gather insights from separate areas of data can help improve the model's predictions.\n     - **Soft Sentinel Mechanism**: This mechanism introduces a form of uncertainty management to the estimations, allowing for the suppression of unreliable predictions from distant references while emphasizing close-enough references for accurate predictions.\n\n5. **Dynamic and Fixed Reference Selection**:\n   - Methods for choosing reference points are flexible and can include fixed precomputed neighbors, random sampling, or dynamic nearest neighbors based on the distance of positions during evaluation. The choice of references is crucial for maintaining the accuracy and efficiency of the model's predictions.\n\n6. **Multiple-Head Architecture**:\n   - Implementing multiple heads allows MAD Learning to utilize outputs from separate instances, facilitating an aggregation mechanism that can enhance representation learning from diverse feature spaces.\n\nOverall, the MAD Learning paradigm innovatively combines memory retention with differential techniques to create a more dynamic, interpretable, and robust method for performing predictions in supervised learning contexts. The design choices underscore an emphasis on leveraging understood relationships and structured memory to enhance model accuracy and efficiency."
}