{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 6,
    "Method": "GIANT-XRT+SAGN+MCR+C&S",
    "External Data": "Yes",
    "Test Accuracy": "0.8673 ± 0.0008",
    "Validation Accuracy": "0.9387 ± 0.0002",
    "Contact": "mailto:yufei.he@bit.edu.cn",
    "Paper Link": "https://arxiv.org/abs/2112.04319",
    "Code Link": "https://github.com/THUDM/CRGNN",
    "Parameters": "1,154,654",
    "Hardware": "GeForce RTX™ 3090 24GB (GPU)",
    "Date": "Dec 8, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GIANT-XRT+SAGN+MCR+C&S.pdf",
    "Paper Summary": "The paper proposes the SCR (Semi-supervised Consistency Regularization) framework designed to enhance the training of Graph Neural Networks (GNNs) by integrating consistency regularization techniques. Here are the main methods discussed, particularly focusing on the model design aspects:\n\n### 1. Overview of SCR Framework:\n\n- **Consistency Regularization:** The SCR framework introduces two key strategies for fostering consistency in predictions from GNNs, particularly when leveraging both labeled and unlabeled graph data.\n\n### 2. Methodological Components:\n\n#### SCR Strategy:\n- **Minimization of Disagreement:** The first strategy focuses on reducing discrepancies in predictions among various perturbed versions of the GNN model. This is achieved by applying slight variations (e.g., data augmentation or random dropout) to the input or model during training.\n- **Pseudolabel Assignment:** Unlabeled nodes receive pseudo labels, which are calculated as the average of noisy predictions derived from S different evaluations of the same input with dropout enabled. This helps align the GNN's predictions across these multiple runs.\n\n#### SCR-m Strategy (Mean-Teacher Consistency Regularization):\n- **Teacher-Student Paradigm:** SCR-m employs a teacher-student framework where a teacher model's parameters are derived via an exponential moving average (EMA) of the student model's parameters, ensuring the teacher remains stable throughout training.\n- **Consistency Loss Calculation:** Instead of averaging the predictions, the consistency loss in SCR-m is framed as the distance between the student’s predictions and those from the teacher model. This setup allows training to align outputs from the student model to the more stable teacher predictions.\n\n### 3. Pseudo Labeling Techniques:\n- In both SCR and SCR-m, the paper describes techniques to generate pseudolabels. SCR uses the mean of noisy outputs to assign pseudolabels whereas SCR-m relies on the teacher's prediction, which is expected to be a more accurate reflection of the model's learned capabilities.\n\n### 4. Network Design and Update Mechanisms:\n- **Noisy Predictions Generation:** The framework introduces randomness through dropout strategies during forward passes to create variations in predictions, which are crucial for evaluating consistency between model outputs.\n- **Loss Function Construction:** The training loss consists of two parts: a conventional supervised loss based on labeled nodes and an unsupervised loss on unlabeled nodes focused on the distance between the pseudolabels and the predictions. This dual-loss approach allows the model to leverage available data more efficiently.\n- **Confidence Masking:** A confidence-based masking strategy is implemented to filter out predictions that are unreliable. Nodes are selected based on a confidence threshold to ensure that only the most confident predictions influence the unsupervised loss calculation.\n\n### 5. Training Algorithm:\n- The SCR framework includes a structured training algorithm that combines the updates from the labeled and unlabeled nodes while managing different parameters such as the consistency weight and EMA decay rate to fine-tune the training process.\n\nThese components collectively aim to balance the contributions from labeled and unlabeled data, ultimately resulting in improved generalization of GNNs in semi-supervised learning contexts. SCR exemplifies a flexible design that can be adapted to various GNN architectures, promoting a more robust training methodology for graph-based tasks."
}