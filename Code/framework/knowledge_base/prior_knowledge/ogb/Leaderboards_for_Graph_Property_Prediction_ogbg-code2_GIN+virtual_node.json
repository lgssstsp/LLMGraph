{
    "Task Description": "Leaderboards for Graph Property Prediction",
    "Dataset Name": "ogbg-code2",
    "Dataset Link": "../graphprop/#ogbg-code2",
    "Rank": 8,
    "Method": "GIN+virtual node",
    "External Data": "No",
    "Test Accuracy": "0.1581 ± 0.0026",
    "Validation Accuracy": "0.1439 ± 0.0020",
    "Contact": "mailto:weihuahu@cs.stanford.edu",
    "Paper Link": "https://arxiv.org/abs/1810.00826",
    "Code Link": "https://github.com/snap-stanford/ogb/tree/master/examples/graphproppred/code2",
    "Parameters": "13,841,815",
    "Hardware": "GeForce RTX 2080 (11GB GPU)",
    "Date": "Feb 24, 2021",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Graph_Property_Prediction/ogbg-code2/GIN+virtual_node.pdf",
    "Paper Summary": "The paper \"How Powerful Are Graph Neural Networks?\" presents a theoretical framework to analyze the expressive power of Graph Neural Networks (GNNs), focusing on model design aspects. Here’s a summary of the key model design methodologies discussed:\n\n1. **Framework for Analysis**: The authors establish a theoretical framework based on the Weisfeiler-Lehman (WL) graph isomorphism test, which helps characterize the discriminative power of various GNNs. GNNs recursively update node representations by aggregating features from neighboring nodes, inspired by the WL test's iterative aggregation of node labels.\n\n2. **Neighbor Aggregation Functionality**: The representation for a node is computed through an aggregation scheme that captures the multiset of its neighbors' features. The expressiveness of a GNN is linked to the ability to uniquely represent these multisets: for a GNN to be powerful, it must map different multisets to different representations. \n\n3. **Injectivity Conditions**: The paper emphasizes that aggregation functions in GNNs should ideally be injective. This property allows the GNN to distinguish different graph structures. By establishing conditions under which neighboring aggregations and graph-level readout functions are injective, the authors demonstrate that such GNN architectures possess the same expressive capability as the WL test.\n\n4. **Graph Isomorphism Network (GIN)**: A significant contribution is the development of the GIN architecture, designed to achieve maximum discriminative power. GIN aggregates features from neighboring nodes with an injective update mechanism:\n   \\[\n   h^{(k)} = \\text{MLP}^{(k)} \\left( (1+\\epsilon^{(k)}) \\cdot h^{(k-1)} + \\sum_{u \\in N(v)} h^{(k-1)}_u \\right)\n   \\]\n   This formula describes updating the node's feature vector, where \\( \\text{MLP}^{(k)} \\) is a multilayer perceptron and \\( N(v) \\) indicates the neighbors of node \\( v \\). The incorporation of \\(\\epsilon^{(k)}\\) adds flexibility to the aggregation mechanism.\n\n5. **Graph-Level Readout Function**: For graph classification tasks, the readout function concatenates node representations across all layers, offering a comprehensive representation of the entire graph structure:\n   \\[\n   h_G = \\text{CONCAT} \\left( \\text{READOUT}\\left( \\{h^{(k)}_v | v \\in G\\}\\right) \\text{ for } k = 0,1,...,K \\right)\n   \\]\n\n6. **Comparison of Aggregation Functions**: The paper critiques various aggregation methods used in existing GNNs—mean and max pooling—highlighting their lack of injectivity and lower expressive power. Sum aggregation is proposed as more effective due to its ability to retain complete multisets rather than compressing information.\n\nIn conclusion, the paper underscores that a GNN's representational power stems from its aggregation strategies, specifically focusing on injectiveness and the structural representation captured through its design choices, while introducing GIN as a framework that maximizes these capacities."
}